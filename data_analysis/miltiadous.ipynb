{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"/home/marta/Documenti/eeg-ml-thesis/\"\n",
    "os.chdir(path)\n",
    "\n",
    "import torch \n",
    "torch.set_num_threads(4) \n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import r_pca \n",
    "import scipy.io\n",
    "from tqdm import tqdm\n",
    "import datetime \n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_reduction(A, tol, comp = 0):\n",
    "  rpca = False\n",
    "  rpca_mu = 0\n",
    "  multiscale_pca = False\n",
    "\n",
    "  assert(len(A.shape) == 2)\n",
    "  dmin = min(A.shape)\n",
    "  if rpca:\n",
    "    r = r_pca.R_pca(A, mu = rpca_mu)\n",
    "    print('Auto tol:', 1e-7 * r.frobenius_norm(r.D), 'used tol:', tol)\n",
    "    print('mu', r.mu, 'lambda', r.lmbda)\n",
    "    L, S = r.fit(tol = tol, max_iter = 10, iter_print = 1)\n",
    "    global norm_s\n",
    "    norm_s = np.linalg.norm(S, ord='fro')  # for debug\n",
    "    print('||A,L,S||:', np.linalg.norm(A, ord='fro'), np.linalg.norm(L, ord='fro'), np.linalg.norm(S, ord='fro'))\n",
    "    #np.savez_compressed('rpca.npz', pre = A, post = L)\n",
    "  elif multiscale_pca:\n",
    "    print('MSPCA...')\n",
    "    #ms = mspca.MultiscalePCA()\n",
    "    #L = ms.fit_transform(A, wavelet_func='sym4', threshold=0.1, scale = True )\n",
    "    print('saving MAT file and calling Matlab...')\n",
    "    scipy.io.savemat('mspca.mat', {'A': A}, do_compression = True)\n",
    "    os.system('matlab -batch \"mspca(\\'mspca.mat\\')\"')\n",
    "    L = scipy.io.loadmat('mspca.mat')['L'] \n",
    "  else:\n",
    "    \n",
    "    L = A\n",
    "  U, lam, V = np.linalg.svd(L, full_matrices = False)  # V is transposed\n",
    "  assert(U.shape == (A.shape[0], dmin) and lam.shape == (dmin,) and V.shape == (dmin, A.shape[1]))\n",
    "  #np.savetxt('singular_values.csv', lam)\n",
    "  lam_trunc = lam[lam > 0.015 * lam[0]]  # magic number\n",
    "  p = comp if comp else len(lam_trunc)\n",
    "  assert(p <= dmin)\n",
    "  print('PCA truncation', dmin, '->', p)\n",
    "  return L, V.T[:,:p]\n",
    "\n",
    "# def reduce_matrix(A, V, PCA_COMPONENTS):\n",
    "#   # (N, w, 16) → (N, 16, w) → ((N*16), w) → compute V\n",
    "#   # (N, 16, w) * V → transpose again last dimensions\n",
    "#   B = np.swapaxes(A, 1, 2)  # (N, 16, w)\n",
    "#   C = B.reshape((-1, B.shape[2]))  # ((N*16), w)\n",
    "#   if V is None:\n",
    "#     L, V = pca_reduction(C, 5e-6, comp = PCA_COMPONENTS)\n",
    "#   B = C @ V  # ((N*16), p)\n",
    "#   B = B.reshape((A.shape[0], A.shape[2], B.shape[1]))  # (N, 16, p)\n",
    "#   return np.swapaxes(B, 1, 2), V  # B = (N, p, 16)\n",
    "\n",
    "# def adjust_size(x, y):\n",
    "#   # when flattening the data matrix on the first dimension, y must be made compatible\n",
    "#   if len(x) == len(y): return y\n",
    "#   factor = len(x) // len(y)\n",
    "#   ynew = np.empty((len(x), 1))\n",
    "#   for i in range(0, len(y)):\n",
    "#     ynew[i * factor : (i + 1) * factor] = y[i]\n",
    "#   return ynew\n",
    "\n",
    "\n",
    "# def oversampling(x_data, y_data, num_classes=2):\n",
    "#   # Duplicate inputs with classes occurring less, so to have a more balanced distribution.\n",
    "#   # It operates on single data windows, so use it on data that have already been split\n",
    "#   #  by subject (typically only on training data).\n",
    "#   x_data_over = x_data.copy()\n",
    "#   y_data_over = y_data.copy()\n",
    "#   occurr = [np.sum(y_data == cl) for cl in range(0, num_classes)]\n",
    "#   for cl in range(0, num_classes):\n",
    "#     if occurr[cl] == max(occurr):\n",
    "#       continue\n",
    "#     mask = y_data[:, 0] == cl\n",
    "#     x_dup = x_data[mask].copy()\n",
    "#     y_dup = y_data[mask].copy()\n",
    "#     while occurr[cl] < max(occurr):\n",
    "#       x_dup_jitter = x_dup + np.random.normal(scale=0.03, size=x_dup.shape)\n",
    "#       how_many = min(len(y_dup), max(occurr) - occurr[cl])\n",
    "#       x_data_over = np.vstack((x_data_over, x_dup_jitter[:how_many]))\n",
    "#       y_data_over = np.vstack((y_data_over, y_dup[:how_many]))\n",
    "#       occurr[cl] += how_many\n",
    "#   return x_data_over, y_data_over   \n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout_prob=0.5, use_dense1=False):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.use_dense1 = use_dense1\n",
    "        if use_dense1:\n",
    "            self.dense1 = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(hidden_dim if use_dense1 else input_dim, hidden_dim, num_layers=num_layers, \n",
    "                             batch_first=True, dropout=dropout_prob if num_layers > 1 else 0, \n",
    "                             bidirectional=False)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout_prob) \n",
    "\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, num_layers=num_layers, batch_first=True, \n",
    "                             dropout=dropout_prob if num_layers > 1 else 0) \n",
    "\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_dense1:\n",
    "            x = self.dense1(x)\n",
    "        \n",
    "        # First LSTM layer\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.dropout1(out)\n",
    "\n",
    "        # Second LSTM layer (keeps last output only)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.dropout2(out[:, -1, :])  # Keep only last timestep\n",
    "        \n",
    "        # Fully connected output\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out  # No softmax, since PyTorch's CrossEntropyLoss applies it\n",
    "\n",
    "def calculate_accuracy(y_pred, y_true):\n",
    "  \n",
    "  correct = (y_pred == y_true).sum().item()\n",
    "  \n",
    "  return correct / y_true.size(0)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "  print(\"starting training loop\")\n",
    "  \n",
    "  \"\"\"\n",
    "  Define Training Step\n",
    "  \"\"\"\n",
    "  \n",
    "  model.train()\n",
    "  \n",
    "  train_loss = 0.0\n",
    "  pred_list = []\n",
    "  gt_list = []\n",
    "  \n",
    "  \n",
    "  for batch_idx, (data, target) in enumerate(tqdm(train_loader)):\n",
    "\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    \n",
    "\n",
    "    target = target.squeeze().long()\n",
    "\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(output, target)\n",
    "    train_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    _, y_pred = torch.max(output,1)\n",
    "    \n",
    "    pred_list.append(y_pred)\n",
    "    gt_list.append(target)\n",
    "    \n",
    "  pred_list = torch.cat(pred_list)\n",
    "  gt_list = torch.cat(gt_list)\n",
    "  \n",
    "  train_acc = calculate_accuracy(pred_list, gt_list) \n",
    "  print(\"ended training step\")\n",
    "  return train_loss / len(train_loader), train_acc, pred_list, gt_list \n",
    "\n",
    "def validation(model, device, val_loader):\n",
    "  print(\"starting validation step\")\n",
    "  \n",
    "  \"\"\"\n",
    "  Define Validation Step\n",
    "  \"\"\"\n",
    "    \n",
    "  model.eval()\n",
    "  \n",
    "  val_loss = 0   \n",
    "  pred_list = []\n",
    "  gt_list = []\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    \n",
    "    for data, target in val_loader:\n",
    "        \n",
    "      data, target = data.to(device), target.to(device).squeeze().long()\n",
    "      output = model(data)\n",
    "      loss = criterion(output, target)\n",
    "      val_loss += loss.item()\n",
    "      _, y_pred = torch.max(output,1)\n",
    "      \n",
    "      pred_list.append(y_pred)\n",
    "      gt_list.append(target)\n",
    "      # correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "      \n",
    "  pred_list = torch.cat(pred_list)\n",
    "  gt_list = torch.cat(gt_list)\n",
    "           \n",
    "  val_acc = calculate_accuracy(pred_list, gt_list)\n",
    "  \n",
    "  \n",
    "  return val_loss / len(val_loader.dataset), val_acc, pred_list, gt_list \n",
    "     \n",
    " \n",
    "def test_and_save_confusion_matrix(model, device, loader,cm_name):\n",
    "    model.eval()\n",
    "    gt_list = []\n",
    "    pred_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device).to(torch.float32), target.to(device).squeeze().long()\n",
    "            output = model(data).float()\n",
    "            _, y_pred = torch.max(output, 1)  \n",
    "            \n",
    "            pred_list.append(y_pred)\n",
    "            gt_list.append(target)\n",
    "            \n",
    "        pred_list = torch.cat(pred_list)\n",
    "        gt_list = torch.cat(gt_list)\n",
    "      \n",
    "    test_acc = calculate_accuracy(pred_list, gt_list)\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}%\")    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(gt_list.cpu().numpy(), pred_list.cpu().numpy())\n",
    "    num_classes = cm.shape[0]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(num_classes))\n",
    "\n",
    "    # Plot and save confusion matrix\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(f'output-milt/{cm_name}')\n",
    "    plt.show()   \n",
    "         \n",
    "def save_model(model, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Function to save model states for a given epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    model_dir = os.path.join(os.getcwd(), \"output-milt\")\n",
    "    os.makedirs(model_dir, exist_ok=True)  # Creates directory if it doesn't exist\n",
    "\n",
    "    # Generate filename with timestamp\n",
    "    now = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    path = os.path.join(model_dir, f\"{model_name}_{now}.pth\")  # Add `.pth` for clarity\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, path)\n",
    "\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def save_best_model(model, optimizer, epoch, path):\n",
    "    \"\"\"\n",
    "    Saves the best model based on validation loss.\n",
    "    Overwrites the existing file if the new model is better.\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, path)\n",
    "\n",
    "    print(f\"Best model saved to {path}\")\n",
    "\n",
    "\n",
    "\n",
    "def subj_list_task(task, df):\n",
    "    \"\"\"Creates list of subjects to load based on the task\"\"\"\n",
    "\n",
    "    subset = df[df[\"Group\"].isin(class_groups[task])]\n",
    "\n",
    "    subject_list = tuple(zip(subset['participant_id'], subset['Group']))\n",
    "\n",
    "    train, test = train_test_split(subject_list, test_size = 0.1, random_state=42, stratify=subset[\"Group\"])\n",
    "\n",
    "    print(f\"Task: {task}\")\n",
    "    print(f\"Number of Subjects in Train set {len(train)}\")\n",
    "    print(f\"Number of Subjects in Train set {len(test)}\")\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def precompute_crops(subject_list, window, overlap, DATASET_DIR, task, train_dataset=None):\n",
    "\n",
    "    base_dir = \"/home/marta/Documenti/eeg-ml-thesis/\"\n",
    "\n",
    "    if train_dataset is True:\n",
    "        save_dir = os.path.join(\"/home/marta/Documenti/data/\", \"miltiadous-train\")\n",
    "        csv_file_name = f\"train_config_file_milt_{task}.csv\"\n",
    "    else:\n",
    "        save_dir = os.path.join(\"home/marta/Documenti/data/\", \"miltiadous-test\")\n",
    "        csv_file_name = f\"test_config_file_milt_{task}.csv\"\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    task_class = class_groups[task]\n",
    "    label_mapping = {cls: i for i, cls in enumerate(task_class)}\n",
    "\n",
    "    print(f\"Class Mapping: {label_mapping}\")\n",
    "    mapping_file = os.path.join(base_dir, \"output-milt\", f\"class_mapping_{task}.txt\")\n",
    "\n",
    "    with open(mapping_file, \"w\") as f:\n",
    "        f.write(f\"Task: {task}\\n\")\n",
    "        f.write(f\"Class mapping: {label_mapping}\\n\")\n",
    "\n",
    "    \n",
    "    all_crops = []\n",
    "\n",
    "    for subject_id, category_label in subject_list:\n",
    "        file_path = f\"{DATASET_DIR}/{category_label}/{subject_id}.npy\"\n",
    "\n",
    "        eeg = np.load(file_path).T\n",
    "        scaler = StandardScaler()\n",
    "        eeg = scaler.fit_transform(eeg)\n",
    "\n",
    "        num_columns = eeg.shape[1]\n",
    "        num_windows = (len(eeg) - window) // (window - overlap) + 1\n",
    "\n",
    "        i = 0\n",
    "        for w in range(num_windows):\n",
    "            x_data = eeg[i:i + window]\n",
    "            i += (window - overlap)\n",
    "\n",
    "            crop_filename = f\"{subject_id}_{category_label}_crop{w}.npz\"\n",
    "            crop_save_path = os.path.join(save_dir, crop_filename)\n",
    "\n",
    "            y_label = label_mapping[category_label]\n",
    "            y_data = np.array([[y_label]])\n",
    "\n",
    "            np.savez(crop_save_path, x_data=x_data, y_data=y_data)\n",
    "\n",
    "            all_crops.append((subject_id, w, category_label,category_label, crop_save_path))\n",
    "\n",
    "    train_ind, val_ind = train_test_split(all_crops, train_size=0.75, random_state=42, shuffle=True)\n",
    "\n",
    "    with open(csv_file_name, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"subject\", \"crops_number\", \"label\", \"split\", \"file_path\"])\n",
    "\n",
    "        for crop in train_ind:\n",
    "            writer.writerow([crop[0], crop[1], crop[2], crop[3], \"train\", crop[4]])\n",
    "        for crop in val_ind:\n",
    "            writer.writerow([crop[0], crop[1], crop[2], crop[3], \"val\", crop[4]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 256\n",
      "window: 64\n",
      "window: 50\n",
      "Task: A_vs_C\n",
      "Number of Subjects in Train set 58\n",
      "Number of Subjects in Train set 7\n",
      "Class Mapping: {'A': 0, 'C': 1}\n",
      "Class Mapping: {'A': 0, 'C': 1}\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = \"/home/marta/Documenti/milt_np_dataset\"\n",
    "WINDOW = 256\n",
    "OVERLAP = WINDOW // 4\n",
    "# OVERLAP = 0\n",
    "PCA_COMPONENTS = 50\n",
    "num_epochs = 20\n",
    "print(f\"window: {WINDOW}\")\n",
    "print(f\"window: {OVERLAP}\")\n",
    "print(f\"window: {PCA_COMPONENTS}\")\n",
    "\n",
    "## CLASSES\n",
    "# A\t\"Alzheimer Disease Group\"\n",
    "# F\t\"Frontotemporal Dementia Group\"\n",
    "# C\t\"Healthy Group\"\n",
    "\n",
    "# Loading data and computing crops\n",
    "df = pd.read_csv(\"/home/marta/Documenti/milt_dataset/datatset/participants.tsv\",sep=\"\\t\")\n",
    "\n",
    "class_groups = {\n",
    "    \"A_vs_C\": [\"A\", \"C\"],\n",
    "    \"A_vs_F\": [\"A\", \"F\"],\n",
    "    \"F_vs_C\": [\"F\", \"C\"],\n",
    "    \"A_vs_F_vs_C\": [\"A\", \"F\", \"C\"]\n",
    "}\n",
    "# function that creates the list of subjects to load based on the task\n",
    "train_subj_list, test_subj_list = subj_list_task(\"A_vs_C\", df)\n",
    "\n",
    "precompute_crops(train_subj_list, window=WINDOW, DATASET_DIR=DATASET_DIR, task=\"A_vs_C\", overlap=OVERLAP, train_dataset=True)\n",
    "precompute_crops(test_subj_list, window=WINDOW, DATASET_DIR=DATASET_DIR, task=\"A_vs_C\", overlap=OVERLAP, train_dataset=False)\n",
    "\n",
    "# Loading crops for oversampling (only training and validation dataset is oversampled)\n",
    "test_path = \"/home/marta/Documenti/data/miltiadous-test\"\n",
    "train_path = \"/home/marta/Documenti/data/miltiadous-train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data for PCA computation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Data: 100%|██████████| 92410/92410 [14:48<00:00, 104.03it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def reduce_matrix(A, V, PCA_COMPONENTS):\n",
    "    # Check the shape of A\n",
    "    if len(A.shape) == 2:\n",
    "        # If A is 2D (w, 16), expand it to (1, w, 16)\n",
    "        A = np.expand_dims(A, axis=0)\n",
    "\n",
    "    # Now A should be 3D: (N, w, 16)\n",
    "    B = np.swapaxes(A, 1, 2)  # Swap axes: (N, 16, w)\n",
    "    C = B.reshape((-1, B.shape[2]))  # Flatten: ((N*16), w)\n",
    "\n",
    "    if V is None:\n",
    "        L, V = pca_reduction(C, 5e-6, comp=PCA_COMPONENTS)\n",
    "\n",
    "    B = C @ V  # Apply PCA: ((N*16), p)\n",
    "    B = B.reshape((A.shape[0], A.shape[2], B.shape[1]))  # Reshape: (N, 16, p)\n",
    "\n",
    "    return np.swapaxes(B, 1, 2), V  # Return: (N, p, 16)\n",
    "\n",
    "\n",
    "def compute_and_save_pca_dataset(csv_file, output_dir, PCA_COMPONENTS):\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure output directory exists\n",
    "\n",
    "    data_info = pd.read_csv(csv_file)\n",
    "    \n",
    "    train_data = []\n",
    "    \n",
    "    print(\"Processing training data for PCA computation...\")\n",
    "    for _, row in tqdm(data_info[data_info[\"split\"] == \"train\"].iterrows(), \n",
    "                       total=len(data_info[data_info[\"split\"] == \"train\"]), \n",
    "                       desc=\"Training Data\"):\n",
    "        npz_data = np.load(row[\"file_path\"])\n",
    "        x_data = npz_data[\"x_data\"]\n",
    "\n",
    "        if len(x_data.shape) == 2:  \n",
    "            x_data = np.expand_dims(x_data, axis=0)  # Convert (w, 16) → (1, w, 16)\n",
    "\n",
    "        x_data = np.swapaxes(x_data, 1, 2)  # (N, 16, w) → (N, w, 16)\n",
    "        x_data = x_data.reshape((-1, x_data.shape[2]))  # ((N*16), w)\n",
    "\n",
    "        train_data.append(x_data)\n",
    "\n",
    "    train_data = np.vstack(train_data)  \n",
    "    _, Vpca = pca_reduction(train_data, tol=5e-6, comp=PCA_COMPONENTS)\n",
    "\n",
    "    print(f\"Computed PCA matrix (Vpca) with shape: {Vpca.shape}\")\n",
    "\n",
    "    print(\"Applying PCA transformation and saving files...\")\n",
    "    for _, row in tqdm(data_info.iterrows(), total=len(data_info), desc=\"Transforming Data\"):\n",
    "        npz_data = np.load(row[\"file_path\"])\n",
    "        x_data = npz_data[\"x_data\"]\n",
    "        y_data = npz_data[\"y_data\"]\n",
    "\n",
    "        x_data, _ = reduce_matrix(x_data, Vpca, PCA_COMPONENTS)\n",
    "\n",
    "        save_path = os.path.join(output_dir, os.path.basename(row[\"file_path\"]))\n",
    "        np.savez_compressed(save_path, x_data=x_data, y_data=y_data)\n",
    "        # print(f\"Saved PCA-transformed file: {save_path}\")\n",
    "\n",
    "    return Vpca\n",
    "\n",
    "def apply_pca_to_dataset(csv_file, output_dir, Vpca, PCA_COMPONENTS):\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)  \n",
    "\n",
    "    data_info = pd.read_csv(csv_file)\n",
    "\n",
    "    for _, row in data_info.iterrows():\n",
    "        npz_data = np.load(row[\"file_path\"])\n",
    "        x_data = npz_data[\"x_data\"]\n",
    "        y_data = npz_data[\"y_data\"]\n",
    "\n",
    "        x_data, _ = reduce_matrix(x_data, Vpca, PCA_COMPONENTS)  \n",
    "\n",
    "        save_path = os.path.join(output_dir, os.path.basename(row[\"file_path\"]))\n",
    "        np.savez_compressed(save_path, x_data=x_data, y_data=y_data)\n",
    "        # print(f\"Saved PCA-transformed test file: {save_path}\")\n",
    "\n",
    "train_csv_file = \"/home/marta/Documenti/eeg-ml-thesis/train_config_file_milt_A_vs_C.csv\"\n",
    "output_train_pca_dir = \"/home/marta/Documenti/data/pca-milt-train/\"\n",
    "test_csv_file = \"/home/marta/Documenti/eeg-ml-thesis/test_config_file_milt_A_vs_C.csv\"  \n",
    "output_test_pca_dir = \"/home/marta/Documenti/data/pca-milt-test\" \n",
    "\n",
    "Vpca = compute_and_save_pca_dataset(train_csv_file, output_train_pca_dir, PCA_COMPONENTS=50)\n",
    "apply_pca_to_dataset(test_csv_file, output_test_pca_dir, Vpca, PCA_COMPONENTS=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 19]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "class PrecomputedPCAEeGDataset(Dataset):\n",
    "    def __init__(self, csv_file, precomputed_dir, transform=None, split=\"train\"):\n",
    "        self.data_info = pd.read_csv(csv_file)\n",
    "        self.data_info = self.data_info[self.data_info[\"split\"] == split].reset_index(drop=True)\n",
    "        self.precomputed_dir = precomputed_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data_info.iloc[idx]\n",
    "        file_name = os.path.basename(row[\"file_path\"])  \n",
    "        file_path = os.path.join(self.precomputed_dir, file_name)  \n",
    "\n",
    "        npz_data = np.load(file_path)\n",
    "        x_data = npz_data[\"x_data\"]\n",
    "        y_data = npz_data[\"y_data\"]\n",
    "\n",
    "        x_data = torch.tensor(x_data, dtype=torch.float32)\n",
    "        y_data = torch.tensor(y_data, dtype=torch.long)\n",
    "\n",
    "        if self.transform:\n",
    "            x_data = self.transform(x_data)\n",
    "\n",
    "        x_data = x_data.view(50, 19)  \n",
    "        y_data = y_data.squeeze()      \n",
    "        return x_data, y_data\n",
    "\n",
    "\n",
    "train_dataset = PrecomputedPCAEeGDataset(csv_file=train_csv_file, precomputed_dir=output_train_pca_dir, split=\"train\")\n",
    "val_dataset = PrecomputedPCAEeGDataset(csv_file=train_csv_file, precomputed_dir=output_train_pca_dir, split=\"val\")\n",
    "test_dataset = PrecomputedPCAEeGDataset(csv_file=train_csv_file, precomputed_dir=output_test_pca_dir, split=\"train\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "for batch_x, batch_y in train_loader:\n",
    "    print(batch_x.shape, batch_y.shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Original dataset size: {X.shape}, Labels distribution: {np.bincount(y.flatten())}\")\n",
    "X_over, y_over = oversampling(X, y)\n",
    "print(f\"Oversampled dataset size: {X_over.shape}, Labels distribution: {np.bincount(y_over.flatten())}\")\n",
    "X_over = torch.tensor(X_over).float()\n",
    "y_over = torch.tensor(y_over).float()\n",
    "# Train, val, test split and apply PCA \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_over, y_over, train_size = 0.75, random_state=42, shuffle=True)\n",
    "print(f\"training data shape: {X_train.shape}\")\n",
    "print(f\"training data shape: {y_train.shape}\")\n",
    "print(f\"validation data shape: {X_val.shape}\")\n",
    "print(f\"validation data shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing epoch number: 1\n",
      "starting training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/361 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:13<00:00, 26.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ended training step\n",
      "Training Accuracy: 0.55% - Loss: 0.6876\n",
      "starting validation step\n",
      "Validation Accuracy: 0.56% - Loss: 0.0214\n",
      "Best model saved to /home/marta/Documenti/eeg-ml-thesis/output/256_64_50.pth\n",
      "Best model updated at epoch 1 with loss 0.0214\n",
      "\n",
      "Processing epoch number: 2\n",
      "starting training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:12<00:00, 28.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ended training step\n",
      "Training Accuracy: 0.72% - Loss: 0.5603\n",
      "starting validation step\n",
      "Validation Accuracy: 0.81% - Loss: 0.0142\n",
      "Best model saved to /home/marta/Documenti/eeg-ml-thesis/output/256_64_50.pth\n",
      "Best model updated at epoch 2 with loss 0.0142\n",
      "\n",
      "Processing epoch number: 3\n",
      "starting training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:12<00:00, 27.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ended training step\n",
      "Training Accuracy: 0.84% - Loss: 0.4163\n",
      "starting validation step\n",
      "Validation Accuracy: 0.87% - Loss: 0.0105\n",
      "Best model saved to /home/marta/Documenti/eeg-ml-thesis/output/256_64_50.pth\n",
      "Best model updated at epoch 3 with loss 0.0105\n",
      "\n",
      "Processing epoch number: 4\n",
      "starting training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:12<00:00, 27.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ended training step\n",
      "Training Accuracy: 0.88% - Loss: 0.3344\n",
      "starting validation step\n",
      "Validation Accuracy: 0.91% - Loss: 0.0081\n",
      "Best model saved to /home/marta/Documenti/eeg-ml-thesis/output/256_64_50.pth\n",
      "Best model updated at epoch 4 with loss 0.0081\n",
      "\n",
      "Processing epoch number: 5\n",
      "starting training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:13<00:00, 27.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ended training step\n",
      "Training Accuracy: 0.93% - Loss: 0.2288\n",
      "starting validation step\n",
      "Validation Accuracy: 0.95% - Loss: 0.0046\n",
      "Best model saved to /home/marta/Documenti/eeg-ml-thesis/output/256_64_50.pth\n",
      "Best model updated at epoch 5 with loss 0.0046\n",
      "\n",
      "Processing epoch number: 6\n",
      "starting training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:12<00:00, 28.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ended training step\n",
      "Training Accuracy: 0.95% - Loss: 0.1532\n",
      "starting validation step\n",
      "Validation Accuracy: 0.97% - Loss: 0.0032\n",
      "Best model saved to /home/marta/Documenti/eeg-ml-thesis/output/256_64_50.pth\n",
      "Best model updated at epoch 6 with loss 0.0032\n",
      "\n",
      "Processing epoch number: 7\n",
      "starting training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:12<00:00, 28.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ended training step\n",
      "Training Accuracy: 0.97% - Loss: 0.1211\n",
      "starting validation step\n",
      "Validation Accuracy: 0.97% - Loss: 0.0027\n",
      "Best model saved to /home/marta/Documenti/eeg-ml-thesis/output/256_64_50.pth\n",
      "Best model updated at epoch 7 with loss 0.0027\n",
      "\n",
      "Processing epoch number: 8\n",
      "starting training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:12<00:00, 28.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ended training step\n",
      "Training Accuracy: 0.97% - Loss: 0.1012\n",
      "starting validation step\n",
      "Validation Accuracy: 0.98% - Loss: 0.0019\n",
      "Best model saved to /home/marta/Documenti/eeg-ml-thesis/output/256_64_50.pth\n",
      "Best model updated at epoch 8 with loss 0.0019\n",
      "\n",
      "Processing epoch number: 9\n",
      "starting training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:12<00:00, 28.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ended training step\n",
      "Training Accuracy: 0.98% - Loss: 0.0818\n",
      "starting validation step\n",
      "Validation Accuracy: 0.98% - Loss: 0.0019\n",
      "Best model saved to /home/marta/Documenti/eeg-ml-thesis/output/256_64_50.pth\n",
      "Best model updated at epoch 9 with loss 0.0019\n",
      "\n",
      "Processing epoch number: 10\n",
      "starting training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:12<00:00, 28.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ended training step\n",
      "Training Accuracy: 0.98% - Loss: 0.0726\n",
      "starting validation step\n",
      "Validation Accuracy: 0.98% - Loss: 0.0025\n",
      "\n",
      "Processing epoch number: 11\n",
      "starting training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:13<00:00, 27.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ended training step\n",
      "Training Accuracy: 0.98% - Loss: 0.0724\n",
      "starting validation step\n",
      "Validation Accuracy: 0.99% - Loss: 0.0016\n",
      "Best model saved to /home/marta/Documenti/eeg-ml-thesis/output/256_64_50.pth\n",
      "Best model updated at epoch 11 with loss 0.0016\n",
      "\n",
      "Processing epoch number: 12\n",
      "starting training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:12<00:00, 27.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ended training step\n",
      "Training Accuracy: 0.99% - Loss: 0.0589\n",
      "starting validation step\n",
      "Validation Accuracy: 0.99% - Loss: 0.0014\n",
      "Best model saved to /home/marta/Documenti/eeg-ml-thesis/output/256_64_50.pth\n",
      "Best model updated at epoch 12 with loss 0.0014\n",
      "\n",
      "Processing epoch number: 13\n",
      "starting training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:14<00:00, 24.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ended training step\n",
      "Training Accuracy: 0.99% - Loss: 0.0524\n",
      "starting validation step\n",
      "Validation Accuracy: 0.99% - Loss: 0.0012\n",
      "Best model saved to /home/marta/Documenti/eeg-ml-thesis/output/256_64_50.pth\n",
      "Best model updated at epoch 13 with loss 0.0012\n",
      "\n",
      "Processing epoch number: 14\n",
      "starting training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:15<00:00, 23.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ended training step\n",
      "Training Accuracy: 0.99% - Loss: 0.0527\n",
      "starting validation step\n",
      "Validation Accuracy: 0.99% - Loss: 0.0011\n",
      "Best model saved to /home/marta/Documenti/eeg-ml-thesis/output/256_64_50.pth\n",
      "Best model updated at epoch 14 with loss 0.0011\n",
      "\n",
      "Processing epoch number: 15\n",
      "starting training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:15<00:00, 22.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ended training step\n",
      "Training Accuracy: 0.99% - Loss: 0.0461\n",
      "starting validation step\n",
      "Validation Accuracy: 0.99% - Loss: 0.0011\n",
      "\n",
      "Processing epoch number: 16\n",
      "starting training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:16<00:00, 21.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ended training step\n",
      "Training Accuracy: 0.99% - Loss: 0.0395\n",
      "starting validation step\n",
      "Validation Accuracy: 0.98% - Loss: 0.0026\n",
      "\n",
      "Processing epoch number: 17\n",
      "starting training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:18<00:00, 19.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ended training step\n",
      "Training Accuracy: 0.99% - Loss: 0.0456\n",
      "starting validation step\n",
      "Validation Accuracy: 0.98% - Loss: 0.0023\n",
      "\n",
      "Processing epoch number: 18\n",
      "starting training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:18<00:00, 19.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ended training step\n",
      "Training Accuracy: 0.99% - Loss: 0.0393\n",
      "starting validation step\n",
      "Validation Accuracy: 0.99% - Loss: 0.0009\n",
      "Best model saved to /home/marta/Documenti/eeg-ml-thesis/output/256_64_50.pth\n",
      "Best model updated at epoch 18 with loss 0.0009\n",
      "\n",
      "Processing epoch number: 19\n",
      "starting training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:19<00:00, 18.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ended training step\n",
      "Training Accuracy: 0.99% - Loss: 0.0337\n",
      "starting validation step\n",
      "Validation Accuracy: 0.99% - Loss: 0.0009\n",
      "\n",
      "Processing epoch number: 20\n",
      "starting training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:19<00:00, 18.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ended training step\n",
      "Training Accuracy: 0.99% - Loss: 0.0313\n",
      "starting validation step\n",
      "Validation Accuracy: 0.99% - Loss: 0.0009\n",
      "\n",
      "Training history saved at /home/marta/Documenti/eeg-ml-thesis/output/256_64_50.npy\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/marta/Documenti/eeg-ml-thesis/data_analysis/miltiadous.ipynb Cell 11\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marta/Documenti/eeg-ml-thesis/data_analysis/miltiadous.ipynb#X15sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mTraining history saved at \u001b[39m\u001b[39m{\u001b[39;00mhistory_file\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marta/Documenti/eeg-ml-thesis/data_analysis/miltiadous.ipynb#X15sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m cm_name \u001b[39m=\u001b[39m file_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.png\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/marta/Documenti/eeg-ml-thesis/data_analysis/miltiadous.ipynb#X15sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m test_and_save_confusion_matrix(model, device, test_loader, cm_name \u001b[39m=\u001b[39m cm_name)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# call model and training\n",
    "input_dim = 19        \n",
    "hidden_dim = 8        \n",
    "output_dim = 2    \n",
    "window_size = 20      \n",
    "dropout_prob = 0.5 \n",
    "device = torch.device(\"cuda\")\n",
    "model = LSTMModel(input_dim, hidden_dim, output_dim, dropout_prob=dropout_prob, use_dense1=False)\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "# scheduler = StepLR(optimizer, step_size=1)\n",
    "best_val_loss = float('inf')  \n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": []\n",
    "}\n",
    "file_name = f\"{WINDOW}_{OVERLAP}_{PCA_COMPONENTS}\"\n",
    "model_name = file_name + \".pth\"\n",
    "best_model_path = os.path.join(os.getcwd(), \"output-milt\", model_name)  \n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"\\nProcessing epoch number: {epoch}\")\n",
    "    train_loss, train_acc, train_preds, train_gts = train(model, device, train_loader, optimizer, epoch)\n",
    "    print(f\"Training Accuracy: {train_acc:.2f}% - Loss: {train_loss:.4f}\")\n",
    "    val_loss, val_acc, val_preds, val_gts = validation(model, device, val_loader)\n",
    "    print(f\"Validation Accuracy: {val_acc:.2f}% - Loss: {val_loss:.4f}\")\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        save_best_model(model, optimizer, epoch, best_model_path)\n",
    "        print(f\"Best model updated at epoch {epoch} with loss {best_val_loss:.4f}\")\n",
    "# Save training history\n",
    "history_name = file_name + \".npy\"\n",
    "history_file = os.path.join(os.getcwd(), \"output-milt\", history_name)\n",
    "np.save(history_file, history)\n",
    "print(f\"\\nTraining history saved at {history_file}\")\n",
    "cm_name = file_name + \".png\"\n",
    "test_and_save_confusion_matrix(model, device, test_loader, cm_name = cm_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
