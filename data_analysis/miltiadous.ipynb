{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"/home/marta/Documenti/eeg-ml-thesis/\"\n",
    "os.chdir(path)\n",
    "\n",
    "import torch \n",
    "torch.set_num_threads(4)  # Limit to 4 CPU threads\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import r_pca \n",
    "import scipy.io\n",
    "from tqdm import tqdm\n",
    "import datetime \n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_crops(subject_list, window, overlap, DATASET_DIR, num_columns=16, train_dataset=None):\n",
    "    \n",
    "\n",
    "    if train_dataset == True:\n",
    "        save_dir = \"/home/marta/Documenti/eeg-ml-thesis/alessandrini-train\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    elif train_dataset == False:\n",
    "        save_dir = \"/home/marta/Documenti/eeg-ml-thesis/alessandrini-test\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for subject_id, category_label in subject_list:\n",
    "        file_path = f\"{DATASET_DIR}/S{subject_id}_{category_label}.npz\"\n",
    "        save_path = f\"{save_dir}/S{subject_id}_{category_label}_crops.npz\"\n",
    "\n",
    "        # if os.path.exists(save_path): \n",
    "        #    print(f\"Skipping {subject_id}, crops already exist.\")\n",
    "        #    continue\n",
    "\n",
    "        eeg = np.load(file_path)['eeg'].T \n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        eeg = scaler.fit_transform(eeg)\n",
    "\n",
    "        num_windows = (len(eeg) - window) // (window - overlap) + 1\n",
    "        x_data = np.empty((num_windows, window, num_columns))\n",
    "\n",
    "        i = 0\n",
    "        for w in range(num_windows):\n",
    "            x_data[w] = eeg[i:i + window]\n",
    "            i += (window - overlap)\n",
    "\n",
    "        y_data = np.full((num_windows, 1), (category_label == 'AD')) \n",
    "\n",
    "        np.savez(save_path, x_data=x_data, y_data=y_data)\n",
    "        # print(f\"Saved crops for {subject_id} at {save_path}\")\n",
    "\n",
    "\n",
    "def load_npz_data(directory):\n",
    "\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".npz\"):  \n",
    "            file_path = os.path.join(directory, file)\n",
    "            data = np.load(file_path)\n",
    "            \n",
    "            x_list.append(data['x_data'])  \n",
    "            y_list.append(data['y_data'])  \n",
    "\n",
    "    x_data = np.vstack(x_list) if x_list else np.array([])\n",
    "    y_data = np.vstack(y_list) if y_list else np.array([])\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "def split_train_val(dataset, test_size=0.2, random_state=42):\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        range(len(dataset.crops_index)), \n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    return train_indices, val_indices \n",
    "\n",
    "def pca_reduction(A, tol, comp = 0):\n",
    "  rpca = False\n",
    "  rpca_mu = 0\n",
    "  multiscale_pca = False\n",
    "\n",
    "  assert(len(A.shape) == 2)\n",
    "  dmin = min(A.shape)\n",
    "  if rpca:\n",
    "    r = r_pca.R_pca(A, mu = rpca_mu)\n",
    "    print('Auto tol:', 1e-7 * r.frobenius_norm(r.D), 'used tol:', tol)\n",
    "    print('mu', r.mu, 'lambda', r.lmbda)\n",
    "    L, S = r.fit(tol = tol, max_iter = 10, iter_print = 1)\n",
    "    global norm_s\n",
    "    norm_s = np.linalg.norm(S, ord='fro')  # for debug\n",
    "    print('||A,L,S||:', np.linalg.norm(A, ord='fro'), np.linalg.norm(L, ord='fro'), np.linalg.norm(S, ord='fro'))\n",
    "    #np.savez_compressed('rpca.npz', pre = A, post = L)\n",
    "  elif multiscale_pca:\n",
    "    print('MSPCA...')\n",
    "    #ms = mspca.MultiscalePCA()\n",
    "    #L = ms.fit_transform(A, wavelet_func='sym4', threshold=0.1, scale = True )\n",
    "    print('saving MAT file and calling Matlab...')\n",
    "    scipy.io.savemat('mspca.mat', {'A': A}, do_compression = True)\n",
    "    os.system('matlab -batch \"mspca(\\'mspca.mat\\')\"')\n",
    "    L = scipy.io.loadmat('mspca.mat')['L'] \n",
    "  else:\n",
    "    \n",
    "    L = A\n",
    "  U, lam, V = np.linalg.svd(L, full_matrices = False)  # V is transposed\n",
    "  assert(U.shape == (A.shape[0], dmin) and lam.shape == (dmin,) and V.shape == (dmin, A.shape[1]))\n",
    "  #np.savetxt('singular_values.csv', lam)\n",
    "  lam_trunc = lam[lam > 0.015 * lam[0]]  # magic number\n",
    "  p = comp if comp else len(lam_trunc)\n",
    "  assert(p <= dmin)\n",
    "  print('PCA truncation', dmin, '->', p)\n",
    "  return L, V.T[:,:p]\n",
    "\n",
    "def reduce_matrix(A, V, PCA_COMPONENTS):\n",
    "  # (N, w, 16) → (N, 16, w) → ((N*16), w) → compute V\n",
    "  # (N, 16, w) * V → transpose again last dimensions\n",
    "  B = np.swapaxes(A, 1, 2)  # (N, 16, w)\n",
    "  C = B.reshape((-1, B.shape[2]))  # ((N*16), w)\n",
    "  if V is None:\n",
    "    L, V = pca_reduction(C, 5e-6, comp = PCA_COMPONENTS)\n",
    "  B = C @ V  # ((N*16), p)\n",
    "  B = B.reshape((A.shape[0], A.shape[2], B.shape[1]))  # (N, 16, p)\n",
    "  return np.swapaxes(B, 1, 2), V  # B = (N, p, 16)\n",
    "\n",
    "def adjust_size(x, y):\n",
    "  # when flattening the data matrix on the first dimension, y must be made compatible\n",
    "  if len(x) == len(y): return y\n",
    "  factor = len(x) // len(y)\n",
    "  ynew = np.empty((len(x), 1))\n",
    "  for i in range(0, len(y)):\n",
    "    ynew[i * factor : (i + 1) * factor] = y[i]\n",
    "  return ynew\n",
    "\n",
    "\n",
    "def oversampling(x_data, y_data, num_classes=2):\n",
    "  # Duplicate inputs with classes occurring less, so to have a more balanced distribution.\n",
    "  # It operates on single data windows, so use it on data that have already been split\n",
    "  #  by subject (typically only on training data).\n",
    "  x_data_over = x_data.copy()\n",
    "  y_data_over = y_data.copy()\n",
    "  occurr = [np.sum(y_data == cl) for cl in range(0, num_classes)]\n",
    "  for cl in range(0, num_classes):\n",
    "    if occurr[cl] == max(occurr):\n",
    "      continue\n",
    "    mask = y_data[:, 0] == cl\n",
    "    x_dup = x_data[mask].copy()\n",
    "    y_dup = y_data[mask].copy()\n",
    "    while occurr[cl] < max(occurr):\n",
    "      x_dup_jitter = x_dup + np.random.normal(scale=0.03, size=x_dup.shape)\n",
    "      how_many = min(len(y_dup), max(occurr) - occurr[cl])\n",
    "      x_data_over = np.vstack((x_data_over, x_dup_jitter[:how_many]))\n",
    "      y_data_over = np.vstack((y_data_over, y_dup[:how_many]))\n",
    "      occurr[cl] += how_many\n",
    "  return x_data_over, y_data_over\n",
    "\n",
    "\n",
    "\n",
    "class AlessandriniEegDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data, PCA_COMPONENTS):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.PCA_COMPONENTS = PCA_COMPONENTS\n",
    "\n",
    "        # Compute PCA once using training data\n",
    "        print(\"Computing PCA matrix for training data...\")\n",
    "        _, self.V_pca = reduce_matrix(self.x_data, None, PCA_COMPONENTS)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x_data[idx]  # Shape: (16, w)\n",
    "        y = self.y_data[idx]\n",
    "\n",
    "        # Apply PCA dynamically per sample\n",
    "        x_pca, _ = reduce_matrix(x[np.newaxis, :, :], self.V_pca, self.PCA_COMPONENTS)  \n",
    "        x_pca = x_pca.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "        return torch.tensor(x_pca, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    \n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout_prob=0.5, use_dense1=False):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        # Optional Dense Layer Before LSTM (matches TensorFlow's `dense1`)\n",
    "        self.use_dense1 = use_dense1\n",
    "        if use_dense1:\n",
    "            self.dense1 = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # First LSTM Layer (returns full sequence if second LSTM exists)\n",
    "        self.lstm1 = nn.LSTM(hidden_dim if use_dense1 else input_dim, hidden_dim, num_layers=num_layers, \n",
    "                             batch_first=True, dropout=dropout_prob if num_layers > 1 else 0, \n",
    "                             bidirectional=False)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout_prob) \n",
    "\n",
    "        # Second LSTM Layer (if present, returns last output)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, num_layers=num_layers, batch_first=True, \n",
    "                             dropout=dropout_prob if num_layers > 1 else 0) \n",
    "\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Fully Connected Output Layer (No Softmax, since CrossEntropyLoss expects logits)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_dense1:\n",
    "            x = self.dense1(x)\n",
    "        \n",
    "        # First LSTM layer\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.dropout1(out)\n",
    "\n",
    "        # Second LSTM layer (keeps last output only)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.dropout2(out[:, -1, :])  # Keep only last timestep\n",
    "        \n",
    "        # Fully connected output\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out  # No softmax, since PyTorch's CrossEntropyLoss applies it\n",
    "\n",
    "def calculate_accuracy(y_pred, y_true):\n",
    "  \n",
    "  correct = (y_pred == y_true).sum().item()\n",
    "  \n",
    "  return correct / y_true.size(0)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "  print(\"starting training loop\")\n",
    "  \n",
    "  \"\"\"\n",
    "  Define Training Step\n",
    "  \"\"\"\n",
    "  \n",
    "  model.train()\n",
    "  \n",
    "  train_loss = 0.0\n",
    "  pred_list = []\n",
    "  gt_list = []\n",
    "  \n",
    "  \n",
    "  for batch_idx, (data, target) in enumerate(tqdm(train_loader)):\n",
    "\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    \n",
    "\n",
    "    target = target.squeeze().long()\n",
    "\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(output, target)\n",
    "    train_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    _, y_pred = torch.max(output,1)\n",
    "    \n",
    "    pred_list.append(y_pred)\n",
    "    gt_list.append(target)\n",
    "    \n",
    "  pred_list = torch.cat(pred_list)\n",
    "  gt_list = torch.cat(gt_list)\n",
    "  \n",
    "  train_acc = calculate_accuracy(pred_list, gt_list) \n",
    "  print(\"ended training step\")\n",
    "  return train_loss / len(train_loader), train_acc, pred_list, gt_list \n",
    "\n",
    "def validation(model, device, val_loader):\n",
    "  print(\"starting validation step\")\n",
    "  \n",
    "  \"\"\"\n",
    "  Define Validation Step\n",
    "  \"\"\"\n",
    "    \n",
    "  model.eval()\n",
    "  \n",
    "  val_loss = 0   \n",
    "  pred_list = []\n",
    "  gt_list = []\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    \n",
    "    for data, target in val_loader:\n",
    "        \n",
    "      data, target = data.to(device), target.to(device).squeeze().long()\n",
    "      output = model(data)\n",
    "      loss = criterion(output, target)\n",
    "      val_loss += loss.item()\n",
    "      _, y_pred = torch.max(output,1)\n",
    "      \n",
    "      pred_list.append(y_pred)\n",
    "      gt_list.append(target)\n",
    "      # correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "      \n",
    "  pred_list = torch.cat(pred_list)\n",
    "  gt_list = torch.cat(gt_list)\n",
    "           \n",
    "  val_acc = calculate_accuracy(pred_list, gt_list)\n",
    "  \n",
    "  \n",
    "  return val_loss / len(val_loader.dataset), val_acc, pred_list, gt_list \n",
    "     \n",
    " \n",
    "def test_and_save_confusion_matrix(model, device, loader,cm_name):\n",
    "    model.eval()\n",
    "    gt_list = []\n",
    "    pred_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device).to(torch.float32), target.to(device).squeeze().long()\n",
    "            output = model(data).float()\n",
    "            _, y_pred = torch.max(output, 1)  \n",
    "            \n",
    "            pred_list.append(y_pred)\n",
    "            gt_list.append(target)\n",
    "            \n",
    "        pred_list = torch.cat(pred_list)\n",
    "        gt_list = torch.cat(gt_list)\n",
    "      \n",
    "    test_acc = calculate_accuracy(pred_list, gt_list)\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}%\")    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(gt_list.cpu().numpy(), pred_list.cpu().numpy())\n",
    "    num_classes = cm.shape[0]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(num_classes))\n",
    "\n",
    "    # Plot and save confusion matrix\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(f'output/{cm_name}')\n",
    "    plt.show()   \n",
    "         \n",
    "def save_model(model, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Function to save model states for a given epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    model_dir = os.path.join(os.getcwd(), \"output\")\n",
    "    os.makedirs(model_dir, exist_ok=True)  # Creates directory if it doesn't exist\n",
    "\n",
    "    # Generate filename with timestamp\n",
    "    now = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    path = os.path.join(model_dir, f\"{model_name}_{now}.pth\")  # Add `.pth` for clarity\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, path)\n",
    "\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def save_best_model(model, optimizer, epoch, path):\n",
    "    \"\"\"\n",
    "    Saves the best model based on validation loss.\n",
    "    Overwrites the existing file if the new model is better.\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, path)\n",
    "\n",
    "    print(f\"Best model saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def subj_list_task(task, df):\n",
    "\n",
    "    class_groups = {\n",
    "        \"A_vs_C\": [\"A\", \"C\"],\n",
    "        \"A_vs_F\": [\"A\", \"F\"],\n",
    "        \"F_vs_C\": [\"F\", \"C\"],\n",
    "        \"A_vs_F_vs_C\": [\"A\", \"F\", \"C\"]\n",
    "    }\n",
    "\n",
    "    subset = df[df[\"Group\"].isin(class_groups[task])]\n",
    "\n",
    "    subject_list = tuple(zip(subset['participant_id'], subset['Group']))\n",
    "\n",
    "    train, test = train_test_split(subject_list, test_size = 0.1, random_state=42, stratify=subset[\"Group\"])\n",
    "\n",
    "    print(f\"Task: {task}\")\n",
    "    print(f\"Number of Subjects in Train set {len(train)}\")\n",
    "    print(f\"Number of Subjects in Train set {len(test)}\")\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 256\n",
      "window: 64\n",
      "window: 50\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = \"/home/marta/Documenti/milt_np_dataset\"\n",
    "WINDOW = 256\n",
    "OVERLAP = WINDOW // 4\n",
    "# OVERLAP = 0\n",
    "PCA_COMPONENTS = 50\n",
    "num_epochs = 20\n",
    "print(f\"window: {WINDOW}\")\n",
    "print(f\"window: {OVERLAP}\")\n",
    "print(f\"window: {PCA_COMPONENTS}\")\n",
    "\n",
    "## CLASSES\n",
    "# A\t\"Alzheimer Disease Group\"\n",
    "# F\t\"Frontotemporal Dementia Group\"\n",
    "# C\t\"Healthy Group\"\n",
    "\n",
    "# Loading data and computing crops\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/home/marta/Documenti/milt_dataset/datatset/participants.tsv\",sep=\"\\t\")\n",
    "\n",
    "train_subj_list, test_subj_list = subj_list_task(\"A_vs_C\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_groups = {\n",
    "    \"A_vs_C\": [\"A\", \"C\"],\n",
    "    \"A_vs_F\": [\"A\", \"F\"],\n",
    "    \"F_vs_C\": [\"F\", \"C\"],\n",
    "    \"A_vs_F_vs_C\": [\"A\", \"F\", \"C\"]\n",
    "}\n",
    "\n",
    "def precompute_crops(subject_list, window, overlap, DATASET_DIR, task, train_dataset=None):\n",
    "\n",
    "    base_dir = \"/home/marta/Documenti/eeg-ml-thesis/\"\n",
    "    \n",
    "\n",
    "    if train_dataset == True:\n",
    "        save_dir = os.path.join(base_dir,\"miltiadous-train\")\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    elif train_dataset == False:\n",
    "        save_dir = os.path.join(base_dir,\"miltiadous-test\")\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    task_class = class_groups[task]\n",
    "    label_mapping = {cls: i for i, cls in enumerate(task_class)}\n",
    "\n",
    "    print(f\"Class Mapping:{label_mapping}\")\n",
    "    mapping_file = os.path.join(base_dir, f\"class_mapping_{task}.txt\")\n",
    "\n",
    "    with open(mapping_file, \"w\") as f:\n",
    "        f.write(f\"Task: {task}\")\n",
    "        f.write(f\"Class mapping: {mapping_file}\")\n",
    "\n",
    "    for subject_id, category_label in subject_list:\n",
    "        file_path = f\"{DATASET_DIR}/{category_label}/{subject_id}.npy\"\n",
    "        save_path = f\"{save_dir}/{subject_id}_{category_label}_crops.npz\"\n",
    "\n",
    "        # if os.path.exists(save_path): \n",
    "        #    print(f\"Skipping {subject_id}, crops already exist.\")\n",
    "        #    continue\n",
    "\n",
    "        eeg = np.load(file_path).T \n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        eeg = scaler.fit_transform(eeg)\n",
    "        num_columns = eeg.shape[1]\n",
    "        num_windows = (len(eeg) - window) // (window - overlap) + 1\n",
    "        x_data = np.empty((num_windows, window, num_columns))\n",
    "\n",
    "        i = 0\n",
    "        for w in range(num_windows):\n",
    "            x_data[w] = eeg[i:i + window]\n",
    "            i += (window - overlap)\n",
    "\n",
    "        ##TODO cambiare come legge i dati per il fatto che voglio tesatre su due o tre classi\n",
    "        y_label = label_mapping[category_label]\n",
    "        y_data = np.full((num_windows, 1), y_label) \n",
    "\n",
    "        np.savez(save_path, x_data=x_data, y_data=y_data)\n",
    "        # print(f\"Saved crops for {subject_id} at {save_path}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Mapping:{'A': 0, 'C': 1}\n"
     ]
    }
   ],
   "source": [
    "precompute_crops(train_subj_list, window=WINDOW, DATASET_DIR=DATASET_DIR, task=\"A_vs_C\", overlap=OVERLAP, train_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NpzFile' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/marta/Documenti/eeg-ml-thesis/data_analysis/miltiadous.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marta/Documenti/eeg-ml-thesis/data_analysis/miltiadous.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39m/home/marta/Documenti/eeg-ml-thesis/miltiadous-train/sub-059_C_crops.npz\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/marta/Documenti/eeg-ml-thesis/data_analysis/miltiadous.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NpzFile' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "test = np.load(\"/home/marta/Documenti/eeg-ml-thesis/miltiadous-train/sub-059_C_crops.npz\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2049, 256, 19)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['x_data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 256\n",
      "window: 64\n",
      "window: 50\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/marta/Documenti/milt_np_dataset/S03_N.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/marta/Documenti/eeg-ml-thesis/data_analysis/miltiadous.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marta/Documenti/eeg-ml-thesis/data_analysis/miltiadous.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m test_subject_list \u001b[39m=\u001b[39m [subj_list[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m subjs_test]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marta/Documenti/eeg-ml-thesis/data_analysis/miltiadous.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m train_val_subjects \u001b[39m=\u001b[39m [subj \u001b[39mfor\u001b[39;00m i, subj \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(subj_list) \u001b[39mif\u001b[39;00m i \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m subjs_test]   \n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/marta/Documenti/eeg-ml-thesis/data_analysis/miltiadous.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m precompute_crops(train_val_subjects, window\u001b[39m=\u001b[39;49mWINDOW, DATASET_DIR\u001b[39m=\u001b[39;49mDATASET_DIR, overlap\u001b[39m=\u001b[39;49mOVERLAP, train_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marta/Documenti/eeg-ml-thesis/data_analysis/miltiadous.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m precompute_crops(test_subject_list, window\u001b[39m=\u001b[39mWINDOW, DATASET_DIR\u001b[39m=\u001b[39mDATASET_DIR, overlap\u001b[39m=\u001b[39mOVERLAP, train_dataset\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m/home/marta/Documenti/eeg-ml-thesis/data_analysis/miltiadous.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marta/Documenti/eeg-ml-thesis/data_analysis/miltiadous.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m save_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00msave_dir\u001b[39m}\u001b[39;00m\u001b[39m/S\u001b[39m\u001b[39m{\u001b[39;00msubject_id\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mcategory_label\u001b[39m}\u001b[39;00m\u001b[39m_crops.npz\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marta/Documenti/eeg-ml-thesis/data_analysis/miltiadous.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# if os.path.exists(save_path): \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marta/Documenti/eeg-ml-thesis/data_analysis/miltiadous.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#    print(f\"Skipping {subject_id}, crops already exist.\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marta/Documenti/eeg-ml-thesis/data_analysis/miltiadous.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m#    continue\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/marta/Documenti/eeg-ml-thesis/data_analysis/miltiadous.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m eeg \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(file_path)[\u001b[39m'\u001b[39m\u001b[39meeg\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mT \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marta/Documenti/eeg-ml-thesis/data_analysis/miltiadous.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marta/Documenti/eeg-ml-thesis/data_analysis/miltiadous.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m eeg \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(eeg)\n",
      "File \u001b[0;32m~/Documenti/eeg-ml-thesis/.venv/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:459\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    457\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os\u001b[39m.\u001b[39;49mfspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    460\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/marta/Documenti/milt_np_dataset/S03_N.npz'"
     ]
    }
   ],
   "source": [
    "precompute_crops(train, window=WINDOW, DATASET_DIR=DATASET_DIR, overlap=OVERLAP, train_dataset=True)\n",
    "precompute_crops(test_subject_list, window=WINDOW, DATASET_DIR=DATASET_DIR, overlap=OVERLAP, train_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loading crops for oversampling (only training and validation dataset is oversampled)\n",
    "test_path = \"/home/marta/Documenti/eeg-ml-thesis/alessandrini-test\"\n",
    "train_path = \"/home/marta/Documenti/eeg-ml-thesis/alessandrini-train\"\n",
    "X_test, y_test = load_npz_data(test_path)\n",
    "X, y = load_npz_data(train_path)\n",
    "print(f\"Original dataset size: {X.shape}, Labels distribution: {np.bincount(y.flatten())}\")\n",
    "X_over, y_over = oversampling(X, y)\n",
    "print(f\"Oversampled dataset size: {X_over.shape}, Labels distribution: {np.bincount(y_over.flatten())}\")\n",
    "X_over = torch.tensor(X_over).float()\n",
    "y_over = torch.tensor(y_over).float()\n",
    "# Train, val, test split and apply PCA \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_over, y_over, train_size = 0.75, random_state=42, shuffle=True)\n",
    "print(f\"training data shape: {X_train.shape}\")\n",
    "print(f\"training data shape: {y_train.shape}\")\n",
    "print(f\"validation data shape: {X_val.shape}\")\n",
    "print(f\"validation data shape: {y_val.shape}\")\n",
    "# X_train, Vpca = reduce_matrix(X_train, None, PCA_COMPONENTS)\n",
    "# y_train = adjust_size(X_train, y_train)\n",
    "# X_val, _ = reduce_matrix(X_val, Vpca, PCA_COMPONENTS)\n",
    "# y_val = adjust_size(X_val, y_val)\n",
    "# X_test, _ = reduce_matrix(X_test, Vpca.cpu().numpy() if isinstance(Vpca, torch.Tensor) else Vpca, PCA_COMPONENTS)\n",
    "# y_test = adjust_size(X_test, y_test).astype(np.float32)\n",
    "# x_data_test = x_data_test.astype(np.float32)\n",
    "# print(f\"training data shape: {X_train.shape}\")\n",
    "# print(f\"training data shape: {y_train.shape}\")\n",
    "# print(f\"validation data shape: {X_val.shape}\")\n",
    "# print(f\"validation data shape: {y_val.shape}\")\n",
    "# Initialize the dataset\n",
    "train_dataset = AlessandriniEegDataset(X_train, y_train, PCA_COMPONENTS)\n",
    "val_dataset = AlessandriniEegDataset(X_val, y_val, PCA_COMPONENTS)\n",
    "test_dataset = AlessandriniEegDataset(X_test, y_test, PCA_COMPONENTS)\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "# Check one batch\n",
    "for x, y in train_loader:\n",
    "    print(\"Batch X shape:\", x.shape)  # Expected: (batch_size, PCA_COMPONENTS, 16)\n",
    "    print(\"Batch Y shape:\", y.shape)  # Expected: (batch_size,)\n",
    "    break\n",
    "# call model and training\n",
    "input_dim = 16        \n",
    "hidden_dim = 8        \n",
    "output_dim = 2    \n",
    "window_size = 20      \n",
    "dropout_prob = 0.5 \n",
    "device = torch.device(\"cuda\")\n",
    "model = LSTMModel(input_dim, hidden_dim, output_dim, dropout_prob=dropout_prob, use_dense1=False)\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "# scheduler = StepLR(optimizer, step_size=1)\n",
    "best_val_loss = float('inf')  \n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": []\n",
    "}\n",
    "file_name = f\"{WINDOW}_{OVERLAP}_{PCA_COMPONENTS}\"\n",
    "model_name = file_name + \".pth\"\n",
    "best_model_path = os.path.join(os.getcwd(), \"output\", model_name)  \n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"\\nProcessing epoch number: {epoch}\")\n",
    "    train_loss, train_acc, train_preds, train_gts = train(model, device, train_loader, optimizer, epoch)\n",
    "    print(f\"Training Accuracy: {train_acc:.2f}% - Loss: {train_loss:.4f}\")\n",
    "    val_loss, val_acc, val_preds, val_gts = validation(model, device, val_loader)\n",
    "    print(f\"Validation Accuracy: {val_acc:.2f}% - Loss: {val_loss:.4f}\")\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        save_best_model(model, optimizer, epoch, best_model_path)\n",
    "        print(f\"Best model updated at epoch {epoch} with loss {best_val_loss:.4f}\")\n",
    "# Save training history\n",
    "history_name = file_name + \".npy\"\n",
    "history_file = os.path.join(os.getcwd(), \"output\", history_name)\n",
    "np.save(history_file, history)\n",
    "print(f\"\\nTraining history saved at {history_file}\")\n",
    "cm_name = file_name + \".png\"\n",
    "test_and_save_confusion_matrix(model, device, test_loader, cm_name = cm_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
