{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"/home/marta/Documenti/eeg-ml-thesis/\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import r_pca \n",
    "import scipy.io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(subject_list, window, overlap, num_columns=16, num_classes=2):\n",
    "\n",
    "    x_data = np.empty((0, window, num_columns))\n",
    "    y_data = np.empty((0, 1))  # Labels\n",
    "    subj_inputs = []  # Tracks number of windows per subject\n",
    "    \n",
    "    dataset_dir = '/home/marta/Documenti/eeg_rnn_repo/rnn-eeg-ad/eeg2'\n",
    "    # print('\\n### Creating dataset')\n",
    "    tot_rows = 0\n",
    "    \n",
    "    # for subject_id, category_label in subject_list:\n",
    "    subject_id = subject_list[0]\n",
    "    category_label = subject_list[1]\n",
    "    \n",
    "    # print(f\"aaaaaaaaaaaaaa{subject_id}\")\n",
    "    # print(f\"bbbbbbbbbbbbb{category_label}\")\n",
    "    subj_inputs.append(0)  # Initialize window count for this subject\n",
    "    \n",
    "    # Load EEG data\n",
    "    file_path = f\"{dataset_dir}/S{subject_id}_{category_label}.npz\"\n",
    "    eeg = np.load(file_path)['eeg'].T  # Transpose if necessary to get [samples, channels]\n",
    "    \n",
    "    # Scale EEG data\n",
    "    scaler = StandardScaler()\n",
    "    eeg = scaler.fit_transform(eeg)\n",
    "    \n",
    "    assert eeg.shape[1] == num_columns, f\"Expected {num_columns} channels, got {eeg.shape[1]}\"\n",
    "    \n",
    "    # Calculate number of windows\n",
    "    num_windows = 0\n",
    "    i = 0\n",
    "    while i + window <= len(eeg):\n",
    "        i += (window - overlap)\n",
    "        num_windows += 1\n",
    "    \n",
    "    # Preallocate x_data for this subject\n",
    "    x_data_part = np.empty((num_windows, window, num_columns))\n",
    "    \n",
    "    # Extract windows\n",
    "    i = 0\n",
    "    for w in range(num_windows):\n",
    "        x_data_part[w] = eeg[i:i + window]\n",
    "        i += (window - overlap)\n",
    "    \n",
    "    # Update x_data and y_data\n",
    "    x_data = np.vstack((x_data, x_data_part))\n",
    "    y_data = np.vstack((y_data, np.full((num_windows, 1), (category_label == 'AD'))))  # Binary label\n",
    "    subj_inputs[-1] = num_windows\n",
    "    tot_rows += len(eeg)\n",
    "    \n",
    "    # print(f\"Total samples: {tot_rows}\")\n",
    "    # print(f\"x_data shape: {x_data.shape}\")\n",
    "    # print(f\"y_data shape: {y_data.shape}\")\n",
    "    # print(f\"Windows per subject: {subj_inputs}\")\n",
    "    # print(f\"Class distribution: {[np.sum(y_data == cl) for cl in range(num_classes)]}\")\n",
    "    \n",
    "    return x_data, y_data, subj_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_reduction(A, tol, comp = 0):\n",
    "  rpca = False\n",
    "  rpca_mu = 0\n",
    "  multiscale_pca = False\n",
    "\n",
    "  assert(len(A.shape) == 2)\n",
    "  dmin = min(A.shape)\n",
    "  if rpca:\n",
    "    r = r_pca.R_pca(A, mu = rpca_mu)\n",
    "    print('Auto tol:', 1e-7 * r.frobenius_norm(r.D), 'used tol:', tol)\n",
    "    print('mu', r.mu, 'lambda', r.lmbda)\n",
    "    L, S = r.fit(tol = tol, max_iter = 10, iter_print = 1)\n",
    "    global norm_s\n",
    "    norm_s = np.linalg.norm(S, ord='fro')  # for debug\n",
    "    print('||A,L,S||:', np.linalg.norm(A, ord='fro'), np.linalg.norm(L, ord='fro'), np.linalg.norm(S, ord='fro'))\n",
    "    #np.savez_compressed('rpca.npz', pre = A, post = L)\n",
    "  elif multiscale_pca:\n",
    "    print('MSPCA...')\n",
    "    #ms = mspca.MultiscalePCA()\n",
    "    #L = ms.fit_transform(A, wavelet_func='sym4', threshold=0.1, scale = True )\n",
    "    print('saving MAT file and calling Matlab...')\n",
    "    scipy.io.savemat('mspca.mat', {'A': A}, do_compression = True)\n",
    "    os.system('matlab -batch \"mspca(\\'mspca.mat\\')\"')\n",
    "    L = scipy.io.loadmat('mspca.mat')['L'] \n",
    "  else:\n",
    "    L = A\n",
    "  U, lam, V = np.linalg.svd(L, full_matrices = False)  # V is transposed\n",
    "  assert(U.shape == (A.shape[0], dmin) and lam.shape == (dmin,) and V.shape == (dmin, A.shape[1]))\n",
    "  #np.savetxt('singular_values.csv', lam)\n",
    "  lam_trunc = lam[lam > 0.015 * lam[0]]  # magic number\n",
    "  p = comp if comp else len(lam_trunc)\n",
    "  assert(p <= dmin)\n",
    "  print('PCA truncation', dmin, '->', p)\n",
    "  return L, V.T[:,:p]\n",
    "\n",
    "def reduce_matrix(A, V):\n",
    "  # (N, w, 16) → (N, 16, w) → ((N*16), w) → compute V\n",
    "  # (N, 16, w) * V → transpose again last dimensions\n",
    "  B = np.swapaxes(A, 1, 2)  # (N, 16, w)\n",
    "  C = B.reshape((-1, B.shape[2]))  # ((N*16), w)\n",
    "  if V is None:\n",
    "    L, V = pca_reduction(C, 5e-6, comp = 50)\n",
    "  B = C @ V  # ((N*16), p)\n",
    "  B = B.reshape((A.shape[0], A.shape[2], B.shape[1]))  # (N, 16, p)\n",
    "  return np.swapaxes(B, 1, 2), V  # B = (N, p, 16)\n",
    "\n",
    "def adjust_size(x, y):\n",
    "  # when flattening the data matrix on the first dimension, y must be made compatible\n",
    "  if len(x) == len(y): return y\n",
    "  factor = len(x) // len(y)\n",
    "  ynew = np.empty((len(x), 1))\n",
    "  for i in range(0, len(y)):\n",
    "    ynew[i * factor : (i + 1) * factor] = y[i]\n",
    "  return ynew\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EegDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 file_paths, \n",
    "                #  labels, \n",
    "                 create_dataset_crop, \n",
    "                 window, \n",
    "                 overlap):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.file_paths = file_paths\n",
    "        # self.labels = labels\n",
    "        self.create_dataset_crop = create_dataset_crop\n",
    "        self.window = window\n",
    "        self.overlap = overlap\n",
    "        \n",
    "        self.crops_index = self._compute_crops_index()\n",
    "    \n",
    "    def _compute_crops_index(self):\n",
    "        crops_index = []\n",
    "        for file_idx, (file_path) in enumerate(self.file_paths):\n",
    "            # print(f\"file_path: {file_path}\")\n",
    "            crops, _, _ = self.create_dataset_crop(file_path, self.window, self.overlap)\n",
    "            \n",
    "            num_crops = len(crops)\n",
    "            \n",
    "            crops_index.extend([(file_idx, crop_idx) for crop_idx in range(num_crops)])\n",
    "            \n",
    "        return crops_index\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.crops_index)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        file_idx, crop_idx = self.crops_index[idx]\n",
    "        file_path = self.file_paths[file_idx]\n",
    "        \n",
    "        crops, labels, _ = self.create_dataset_crop(file_path, self.window, self.overlap)\n",
    "        x_data_reduced, Vpca = reduce_matrix(crops, None)\n",
    "        labels = adjust_size(x_data_reduced, labels)\n",
    "        # print(np.unique(label[0]))\n",
    "        # print(label.shape)\n",
    "        crop = x_data_reduced[crop_idx]\n",
    "        label = labels[0] \n",
    "        \n",
    "        label = torch.tensor(label).float().squeeze().unsqueeze(0)        \n",
    "        # label = self.labels[file_idx]\n",
    "        \n",
    "        return torch.tensor(crop).float(), label\n",
    "\n",
    "subj_list = (\n",
    "      tuple((f'{i:02d}', 'N') for i in range(1, 16)) +  # normal subjects, S01 to S15\n",
    "      tuple((f'{i:02d}', 'AD') for i in range(1, 21))   # alzheimer's subjects, S01 to S20\n",
    "  )\n",
    "\n",
    "subjs_test = (0, 1, 15, 16, 17)  \n",
    "\n",
    "test_subject_list = [subj_list[i] for i in subjs_test]\n",
    "train_val_subjects = [subj for i, subj in enumerate(subj_list) if i not in subjs_test]   \n",
    "    \n",
    "dataset = EegDataset(file_paths=train_val_subjects,\n",
    "                     create_dataset_crop=create_dataset,\n",
    "                     window=128,\n",
    "                     overlap=25)\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size = 32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.EegDataset at 0x72b241ab1c30>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "PCA truncation 128 -> 50\n",
      "torch.Size([32, 50, 16])\n",
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "for X, y in data_loader:\n",
    "    print(f\"{X.shape}\")\n",
    "    print(f\"{y.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['03', 'N']\n"
     ]
    }
   ],
   "source": [
    "test = ('03', 'N')\n",
    "prova = list(test)\n",
    "\n",
    "print(prova)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('03', 'N')\n",
      "1 ('04', 'N')\n",
      "2 ('05', 'N')\n",
      "3 ('06', 'N')\n",
      "4 ('07', 'N')\n",
      "5 ('08', 'N')\n",
      "6 ('09', 'N')\n",
      "7 ('10', 'N')\n",
      "8 ('11', 'N')\n",
      "9 ('12', 'N')\n",
      "10 ('13', 'N')\n",
      "11 ('14', 'N')\n",
      "12 ('15', 'N')\n",
      "13 ('04', 'AD')\n",
      "14 ('05', 'AD')\n",
      "15 ('06', 'AD')\n",
      "16 ('07', 'AD')\n",
      "17 ('08', 'AD')\n",
      "18 ('09', 'AD')\n",
      "19 ('10', 'AD')\n",
      "20 ('11', 'AD')\n",
      "21 ('12', 'AD')\n",
      "22 ('13', 'AD')\n",
      "23 ('14', 'AD')\n",
      "24 ('15', 'AD')\n",
      "25 ('16', 'AD')\n",
      "26 ('17', 'AD')\n",
      "27 ('18', 'AD')\n",
      "28 ('19', 'AD')\n",
      "29 ('20', 'AD')\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_val_subjects):\n",
    "    print(i, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# normal_subjects = [subj for subj in train_val_subjects if subj[1] == 'N']\n",
    "# ad_subjects = [subj for subj in train_val_subjects if subj[1] == 'AD']\n",
    "\n",
    "# random.seed(42)  \n",
    "# random.shuffle(normal_subjects)\n",
    "# random.shuffle(ad_subjects)\n",
    "\n",
    "# split_index_normal = int(0.8 * len(normal_subjects))\n",
    "# split_index_ad = int(0.8 * len(ad_subjects))\n",
    "\n",
    "# train_normal = normal_subjects[:split_index_normal]\n",
    "# val_normal = normal_subjects[split_index_normal:]\n",
    "\n",
    "# train_ad = ad_subjects[:split_index_ad]\n",
    "# val_ad = ad_subjects[split_index_ad:]\n",
    "\n",
    "# train_subject_list = train_normal + train_ad\n",
    "# val_subject_list = val_normal + val_ad\n",
    "\n",
    "# random.shuffle(train_subject_list)\n",
    "# random.shuffle(val_subject_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Creating dataset\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/marta/Documenti/eeg_rnn_repo/rnn-eeg-ad/eeg2/S0_3.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mEegDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_val_subjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mcreate_dataset_crop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 17\u001b[0m, in \u001b[0;36mEegDataset.__init__\u001b[0;34m(self, file_paths, create_dataset_crop, window, overlap)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow \u001b[38;5;241m=\u001b[39m window\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverlap \u001b[38;5;241m=\u001b[39m overlap\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrops_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_crops_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 23\u001b[0m, in \u001b[0;36mEegDataset._compute_crops_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     20\u001b[0m crops_index \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_idx, file_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_paths):\n\u001b[0;32m---> 23\u001b[0m     crops, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverlap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     num_crops \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(crops)\n\u001b[1;32m     27\u001b[0m     crops_index\u001b[38;5;241m.\u001b[39mextend([(file_idx, crop_idx) \u001b[38;5;28;01mfor\u001b[39;00m crop_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_crops)])\n",
      "Cell \u001b[0;32mIn[14], line 16\u001b[0m, in \u001b[0;36mcreate_dataset\u001b[0;34m(subject_list, window, overlap, num_columns, num_classes)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Load EEG data\u001b[39;00m\n\u001b[1;32m     15\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/S\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m eeg \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meeg\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mT  \u001b[38;5;66;03m# Transpose if necessary to get [samples, channels]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Scale EEG data\u001b[39;00m\n\u001b[1;32m     19\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "File \u001b[0;32m~/Documenti/eeg-ml-thesis/.venv/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:459\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    457\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    460\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/marta/Documenti/eeg_rnn_repo/rnn-eeg-ad/eeg2/S0_3.npz'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
