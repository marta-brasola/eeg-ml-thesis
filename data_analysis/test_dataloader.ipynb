{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"/home/marta/Documenti/eeg-ml-thesis/\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import r_pca \n",
    "import scipy.io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(subject_list, window, overlap, num_columns=16, num_classes=2):\n",
    "\n",
    "    x_data = np.empty((0, window, num_columns))\n",
    "    y_data = np.empty((0, 1))  # Labels\n",
    "    subj_inputs = []  # Tracks number of windows per subject\n",
    "    \n",
    "    dataset_dir = '/home/marta/Documenti/eeg_rnn_repo/rnn-eeg-ad/eeg2'\n",
    "    # print('\\n### Creating dataset')\n",
    "    tot_rows = 0\n",
    "    \n",
    "    # for subject_id, category_label in subject_list:\n",
    "    subject_id = subject_list[0]\n",
    "    category_label = subject_list[1]\n",
    "    \n",
    "    # print(f\"aaaaaaaaaaaaaa{subject_id}\")\n",
    "    # print(f\"bbbbbbbbbbbbb{category_label}\")\n",
    "    subj_inputs.append(0)  # Initialize window count for this subject\n",
    "    \n",
    "    # Load EEG data\n",
    "    file_path = f\"{dataset_dir}/S{subject_id}_{category_label}.npz\"\n",
    "    eeg = np.load(file_path)['eeg'].T  # Transpose if necessary to get [samples, channels]\n",
    "    \n",
    "    # Scale EEG data\n",
    "    scaler = StandardScaler()\n",
    "    eeg = scaler.fit_transform(eeg)\n",
    "    \n",
    "    assert eeg.shape[1] == num_columns, f\"Expected {num_columns} channels, got {eeg.shape[1]}\"\n",
    "    \n",
    "    # Calculate number of windows\n",
    "    num_windows = 0\n",
    "    i = 0\n",
    "    while i + window <= len(eeg):\n",
    "        i += (window - overlap)\n",
    "        num_windows += 1\n",
    "    \n",
    "    # Preallocate x_data for this subject\n",
    "    x_data_part = np.empty((num_windows, window, num_columns))\n",
    "    \n",
    "    # Extract windows\n",
    "    i = 0\n",
    "    for w in range(num_windows):\n",
    "        x_data_part[w] = eeg[i:i + window]\n",
    "        i += (window - overlap)\n",
    "    \n",
    "    # Update x_data and y_data\n",
    "    x_data = np.vstack((x_data, x_data_part))\n",
    "    y_data = np.vstack((y_data, np.full((num_windows, 1), (category_label == 'AD'))))  # Binary label\n",
    "    subj_inputs[-1] = num_windows\n",
    "    tot_rows += len(eeg)\n",
    "    \n",
    "    # print(f\"Total samples: {tot_rows}\")\n",
    "    # print(f\"x_data shape: {x_data.shape}\")\n",
    "    # print(f\"y_data shape: {y_data.shape}\")\n",
    "    # print(f\"Windows per subject: {subj_inputs}\")\n",
    "    # print(f\"Class distribution: {[np.sum(y_data == cl) for cl in range(num_classes)]}\")\n",
    "    \n",
    "    return x_data, y_data, subj_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EegDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 file_paths, \n",
    "                #  labels, \n",
    "                 create_dataset_crop, \n",
    "                 window, \n",
    "                 overlap):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.file_paths = file_paths\n",
    "        # self.labels = labels\n",
    "        self.create_dataset_crop = create_dataset_crop\n",
    "        self.window = window\n",
    "        self.overlap = overlap\n",
    "        \n",
    "        self.crops_index = self._compute_crops_index()\n",
    "    \n",
    "    def _compute_crops_index(self):\n",
    "        crops_index = []\n",
    "        for file_idx, (file_path) in enumerate(self.file_paths):\n",
    "            # print(f\"file_path: {file_path}\")\n",
    "            crops, _, _ = self.create_dataset_crop(file_path, self.window, self.overlap)\n",
    "            \n",
    "            num_crops = len(crops)\n",
    "            \n",
    "            crops_index.extend([(file_idx, crop_idx) for crop_idx in range(num_crops)])\n",
    "            \n",
    "        return crops_index\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.crops_index)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        file_idx, crop_idx = self.crops_index[idx]\n",
    "        file_path = self.file_paths[file_idx]\n",
    "        \n",
    "        crops, labels, _ = self.create_dataset_crop(file_path, self.window, self.overlap)\n",
    "        x_data_reduced, Vpca = reduce_matrix(crops, None)\n",
    "        labels = adjust_size(x_data_reduced, labels)\n",
    "        # print(np.unique(label[0]))\n",
    "        # print(label.shape)\n",
    "        crop = x_data_reduced[crop_idx]\n",
    "        label = labels[0] \n",
    "        \n",
    "        label = torch.tensor(label).float().squeeze().unsqueeze(0)        \n",
    "        # label = self.labels[file_idx]\n",
    "        \n",
    "        return torch.tensor(crop).float(), label\n",
    "    \n",
    "\n",
    "\n",
    "# create_dataset\n",
    "\n",
    "# apply pca on the crops\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = EegDataset(file_paths=train_val_subjects,\n",
    "                    create_dataset_crop=create_dataset,\n",
    "                    window=128,\n",
    "                    overlap=25)\n",
    "\n",
    "test_dataset = EegDataset(file_paths=test_subject_list,\n",
    "                    create_dataset_crop=create_dataset,\n",
    "                    window=128,\n",
    "                    overlap=25)\n",
    "\n",
    "train_indices, val_indices = split_train_val(dataset, test_size=0.2)\n",
    "train_loader = DataLoader(Subset(dataset, train_indices), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(Subset(dataset, val_indices), batch_size=32, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(window, overlap, decimation_factor = 0):\n",
    "  # Create the input and target data from dataset,\n",
    "  # according to window and overlap\n",
    "  # new dataset 4 dec 2021\n",
    "  # 15 N, 20 AD (resulting indexes: N = 0..14, AD = 15..34)\n",
    "  #Common signals: ['EEG Fp1', 'EEG Fp2', 'EEG F7', 'EEG F3', 'EEG F4', 'EEG F8', 'EEG T3', 'EEG C3', 'EEG C4', 'EEG T4', 'EEG T5', 'EEG P3', 'EEG P4', 'EEG T6', 'EEG O1', 'EEG O2']\n",
    "\n",
    "  np.random.seed(42)\n",
    "  dataset_dir = '/home/marta/Documenti/eeg_rnn_repo/rnn-eeg-ad/eeg2'\n",
    "  subj_list = tuple((f'{i:02d}', 'N') for i in range(1, 16)) + tuple((f'{i:02d}', 'AD') for i in range(1, 21))\n",
    "  print(subj_list)\n",
    "  num_columns = 16\n",
    "\n",
    "  x_data = np.empty((0, window, num_columns))\n",
    "  y_data = np.empty((0, 1))  # labels\n",
    "  subj_inputs = []  # number of inputs for every subject\n",
    "  print('\\n### creating dataset')\n",
    "  tot_rows = 0\n",
    "  for subject in subj_list:\n",
    "    subj_inputs.append(0)\n",
    "    category = ('N', 'AD').index(subject[1])\n",
    "    eeg = np.load(f'{dataset_dir}/S{subject[0]}_{subject[1]}.npz')['eeg'].T\n",
    "    # if spikes: eeg = set_holes(eeg, spikes)\n",
    "    #scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = StandardScaler()\n",
    "    eeg = scaler.fit_transform(eeg)\n",
    "    assert(eeg.shape[1] == num_columns)\n",
    "    tot_rows += len(eeg)\n",
    "    # decimation (optional)\n",
    "    if decimation_factor:\n",
    "      eeg2 = np.empty((eeg.shape[0] // decimation_factor, eeg.shape[1]))\n",
    "      for col in range(0, num_columns):\n",
    "        #tmp = scipy.signal.decimate(fusion[:, col], decimation_factor)\n",
    "        tmp = eeg[:, col][::decimation_factor]  # simpler method\n",
    "        eeg2[:, col] = tmp[:len(eeg2)]\n",
    "      eeg = eeg2\n",
    "    # windowing\n",
    "    # compute number of windows (lazy way)\n",
    "    i = 0\n",
    "    num_w = 0\n",
    "    while i + window  <= len(eeg):\n",
    "      i += (window - overlap)\n",
    "      num_w += 1\n",
    "    # compute actual windows\n",
    "    x_data_part = np.empty((num_w, window, num_columns))  # preallocate\n",
    "    i = 0\n",
    "    for w in range(0, num_w):\n",
    "      x_data_part[w] = eeg[i:i + window]\n",
    "      i += (window - overlap)\n",
    "      if False: # watermark provenience of every window\n",
    "        for cc in range(0, num_columns):\n",
    "          x_data_part[w, 0, cc] = 1000 * (len(subj_inputs) - 1) + cc\n",
    "    x_data = np.vstack((x_data, x_data_part))\n",
    "    y_data = np.vstack((y_data, np.full((num_w, 1), category)))\n",
    "    subj_inputs[-1] += num_w\n",
    "\n",
    "  print('\\ntot samples:', tot_rows)\n",
    "  print('x_data:', x_data.shape)\n",
    "  print('y_data:', y_data.shape)\n",
    "  print('windows per subject:', subj_inputs)\n",
    "  #print('class distribution:', [np.sum(y_data == cl) for cl in range(0, num_classes)])\n",
    "\n",
    "  return x_data, y_data, subj_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversampling(x_data, y_data, num_classes=2):\n",
    "  # Duplicate inputs with classes occurring less, so to have a more balanced distribution.\n",
    "  # It operates on single data windows, so use it on data that have already been split\n",
    "  #  by subject (typically only on training data).\n",
    "  x_data_over = x_data.copy()\n",
    "  y_data_over = y_data.copy()\n",
    "  occurr = [np.sum(y_data == cl) for cl in range(0, num_classes)]\n",
    "  for cl in range(0, num_classes):\n",
    "    if occurr[cl] == max(occurr):\n",
    "      continue\n",
    "    mask = y_data[:, 0] == cl\n",
    "    x_dup = x_data[mask].copy()\n",
    "    y_dup = y_data[mask].copy()\n",
    "    while occurr[cl] < max(occurr):\n",
    "      x_dup_jitter = x_dup + np.random.normal(scale=0.03, size=x_dup.shape)\n",
    "      how_many = min(len(y_dup), max(occurr) - occurr[cl])\n",
    "      x_data_over = np.vstack((x_data_over, x_dup_jitter[:how_many]))\n",
    "      y_data_over = np.vstack((y_data_over, y_dup[:how_many]))\n",
    "      occurr[cl] += how_many\n",
    "  return x_data_over, y_data_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE DATASET FOR ALESSANDRINI \n",
    "\n",
    "subjs_train_perm = ( (tuple(i for i in range(2, 15)) + tuple(i for i in range(18, 35)), ()), )\n",
    "subjs_test = (0, 1, 15, 16, 17)  # 2 for N, 3 for AD\n",
    "#x, y, subjs = create_dataset(train_val_subjects, 128, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codice da usare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved crops for 03 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S03_N_crops.npz\n",
      "Saved crops for 04 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S04_N_crops.npz\n",
      "Saved crops for 05 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S05_N_crops.npz\n",
      "Saved crops for 06 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S06_N_crops.npz\n",
      "Saved crops for 07 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S07_N_crops.npz\n",
      "Saved crops for 08 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S08_N_crops.npz\n",
      "Saved crops for 09 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S09_N_crops.npz\n",
      "Saved crops for 10 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S10_N_crops.npz\n",
      "Saved crops for 11 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S11_N_crops.npz\n",
      "Saved crops for 12 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S12_N_crops.npz\n",
      "Saved crops for 13 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S13_N_crops.npz\n",
      "Saved crops for 14 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S14_N_crops.npz\n",
      "Saved crops for 15 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S15_N_crops.npz\n",
      "Saved crops for 04 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S04_AD_crops.npz\n",
      "Saved crops for 05 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S05_AD_crops.npz\n",
      "Saved crops for 06 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S06_AD_crops.npz\n",
      "Saved crops for 07 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S07_AD_crops.npz\n",
      "Saved crops for 08 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S08_AD_crops.npz\n",
      "Saved crops for 09 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S09_AD_crops.npz\n",
      "Saved crops for 10 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S10_AD_crops.npz\n",
      "Saved crops for 11 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S11_AD_crops.npz\n",
      "Saved crops for 12 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S12_AD_crops.npz\n",
      "Saved crops for 13 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S13_AD_crops.npz\n",
      "Saved crops for 14 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S14_AD_crops.npz\n",
      "Saved crops for 15 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S15_AD_crops.npz\n",
      "Saved crops for 16 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S16_AD_crops.npz\n",
      "Saved crops for 17 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S17_AD_crops.npz\n",
      "Saved crops for 18 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S18_AD_crops.npz\n",
      "Saved crops for 19 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S19_AD_crops.npz\n",
      "Saved crops for 20 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-train/S20_AD_crops.npz\n",
      "Saved crops for 01 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-test/S01_N_crops.npz\n",
      "Saved crops for 02 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-test/S02_N_crops.npz\n",
      "Saved crops for 01 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-test/S01_AD_crops.npz\n",
      "Saved crops for 02 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-test/S02_AD_crops.npz\n",
      "Saved crops for 03 at /home/marta/Documenti/eeg-ml-thesis/alessandrini-test/S03_AD_crops.npz\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = \"/home/marta/Documenti/eeg_rnn_repo/rnn-eeg-ad/eeg2\"\n",
    "# save_dir = \"/home/marta/Documenti/eeg-ml-thesis/output\"\n",
    "\n",
    "def precompute_crops(subject_list, window, overlap, num_columns=16, train_dataset=None):\n",
    "    \n",
    "\n",
    "    if train_dataset == True:\n",
    "        save_dir = \"/home/marta/Documenti/eeg-ml-thesis/alessandrini-train\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    elif train_dataset == False:\n",
    "        save_dir = \"/home/marta/Documenti/eeg-ml-thesis/alessandrini-test\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for subject_id, category_label in subject_list:\n",
    "        file_path = f\"{DATASET_DIR}/S{subject_id}_{category_label}.npz\"\n",
    "        save_path = f\"{save_dir}/S{subject_id}_{category_label}_crops.npz\"\n",
    "\n",
    "        # if os.path.exists(save_path): \n",
    "        #    print(f\"Skipping {subject_id}, crops already exist.\")\n",
    "        #    continue\n",
    "\n",
    "        eeg = np.load(file_path)['eeg'].T \n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        eeg = scaler.fit_transform(eeg)\n",
    "\n",
    "        num_windows = (len(eeg) - window) // (window - overlap) + 1\n",
    "        x_data = np.empty((num_windows, window, num_columns))\n",
    "\n",
    "        i = 0\n",
    "        for w in range(num_windows):\n",
    "            x_data[w] = eeg[i:i + window]\n",
    "            i += (window - overlap)\n",
    "\n",
    "        y_data = np.full((num_windows, 1), (category_label == 'AD')) \n",
    "\n",
    "        np.savez(save_path, x_data=x_data, y_data=y_data)\n",
    "        # print(f\"Saved crops for {subject_id} at {save_path}\")\n",
    "\n",
    "window = 256\n",
    "overlap = window // 2\n",
    "subj_list = (\n",
    "    tuple((f'{i:02d}', 'N') for i in range(1, 16)) +  # normal subjects, S01 to S15\n",
    "    tuple((f'{i:02d}', 'AD') for i in range(1, 21))   # alzheimer's subjects, S01 to S20\n",
    ")\n",
    "subjs_test = (0, 1, 15, 16, 17)  \n",
    "test_subject_list = [subj_list[i] for i in subjs_test]\n",
    "train_val_subjects = [subj for i, subj in enumerate(subj_list) if i not in subjs_test]   \n",
    "precompute_crops(train_val_subjects, window=256, overlap=overlap, train_dataset=True)\n",
    "precompute_crops(test_subject_list, window=256, overlap=overlap, train_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val(dataset, test_size=0.2, random_state=42):\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        range(len(dataset.crops_index)), \n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    return train_indices, val_indices \n",
    "\n",
    "def pca_reduction(A, tol, comp = 0):\n",
    "  rpca = False\n",
    "  rpca_mu = 0\n",
    "  multiscale_pca = False\n",
    "\n",
    "  assert(len(A.shape) == 2)\n",
    "  dmin = min(A.shape)\n",
    "  if rpca:\n",
    "    r = r_pca.R_pca(A, mu = rpca_mu)\n",
    "    print('Auto tol:', 1e-7 * r.frobenius_norm(r.D), 'used tol:', tol)\n",
    "    print('mu', r.mu, 'lambda', r.lmbda)\n",
    "    L, S = r.fit(tol = tol, max_iter = 10, iter_print = 1)\n",
    "    global norm_s\n",
    "    norm_s = np.linalg.norm(S, ord='fro')  # for debug\n",
    "    print('||A,L,S||:', np.linalg.norm(A, ord='fro'), np.linalg.norm(L, ord='fro'), np.linalg.norm(S, ord='fro'))\n",
    "    #np.savez_compressed('rpca.npz', pre = A, post = L)\n",
    "  elif multiscale_pca:\n",
    "    print('MSPCA...')\n",
    "    #ms = mspca.MultiscalePCA()\n",
    "    #L = ms.fit_transform(A, wavelet_func='sym4', threshold=0.1, scale = True )\n",
    "    print('saving MAT file and calling Matlab...')\n",
    "    scipy.io.savemat('mspca.mat', {'A': A}, do_compression = True)\n",
    "    os.system('matlab -batch \"mspca(\\'mspca.mat\\')\"')\n",
    "    L = scipy.io.loadmat('mspca.mat')['L'] \n",
    "  else:\n",
    "    \n",
    "    L = A\n",
    "  U, lam, V = np.linalg.svd(L, full_matrices = False)  # V is transposed\n",
    "  assert(U.shape == (A.shape[0], dmin) and lam.shape == (dmin,) and V.shape == (dmin, A.shape[1]))\n",
    "  #np.savetxt('singular_values.csv', lam)\n",
    "  lam_trunc = lam[lam > 0.015 * lam[0]]  # magic number\n",
    "  p = comp if comp else len(lam_trunc)\n",
    "  assert(p <= dmin)\n",
    "  #print('PCA truncation', dmin, '->', p)\n",
    "  return L, V.T[:,:p]\n",
    "\n",
    "def reduce_matrix(A, V):\n",
    "  # (N, w, 16) → (N, 16, w) → ((N*16), w) → compute V\n",
    "  # (N, 16, w) * V → transpose again last dimensions\n",
    "  B = np.swapaxes(A, 1, 2)  # (N, 16, w)\n",
    "  C = B.reshape((-1, B.shape[2]))  # ((N*16), w)\n",
    "  if V is None:\n",
    "    L, V = pca_reduction(C, 5e-6, comp = 50)\n",
    "  B = C @ V  # ((N*16), p)\n",
    "  B = B.reshape((A.shape[0], A.shape[2], B.shape[1]))  # (N, 16, p)\n",
    "  return np.swapaxes(B, 1, 2), V  # B = (N, p, 16)\n",
    "\n",
    "def adjust_size(x, y):\n",
    "  # when flattening the data matrix on the first dimension, y must be made compatible\n",
    "  if len(x) == len(y): return y\n",
    "  factor = len(x) // len(y)\n",
    "  ynew = np.empty((len(x), 1))\n",
    "  for i in range(0, len(y)):\n",
    "    ynew[i * factor : (i + 1) * factor] = y[i]\n",
    "  return ynew\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S07_N_crops.npz\n",
      "S05_N_crops.npz\n",
      "S11_N_crops.npz\n",
      "S04_AD_crops.npz\n",
      "S06_N_crops.npz\n",
      "S12_N_crops.npz\n",
      "S11_AD_crops.npz\n",
      "S10_AD_crops.npz\n",
      "S05_AD_crops.npz\n",
      "S09_N_crops.npz\n",
      "S09_AD_crops.npz\n",
      "S08_N_crops.npz\n",
      "S06_AD_crops.npz\n",
      "S18_AD_crops.npz\n",
      "S15_AD_crops.npz\n",
      "S15_N_crops.npz\n",
      "S04_N_crops.npz\n",
      "S10_N_crops.npz\n",
      "S12_AD_crops.npz\n",
      "S07_AD_crops.npz\n",
      "S03_N_crops.npz\n",
      "S13_AD_crops.npz\n",
      "S13_N_crops.npz\n",
      "S20_AD_crops.npz\n",
      "S16_AD_crops.npz\n",
      "S08_AD_crops.npz\n",
      "S14_AD_crops.npz\n",
      "S17_AD_crops.npz\n",
      "S19_AD_crops.npz\n",
      "S14_N_crops.npz\n",
      "Original dataset size: (40602, 256, 16), Labels distribution: [16290 24312]\n",
      "Oversampled dataset size: (48624, 256, 16), Labels distribution: [24312 24312]\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/marta/Documenti/eeg-ml-thesis/alessandrini-train\"\n",
    "\n",
    "x_all = []\n",
    "y_all = []\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(\".npz\"):  \n",
    "        file_path = os.path.join(path, file)\n",
    "        data = np.load(file_path)\n",
    "        \n",
    "        x_all.append(data['x_data'])  \n",
    "        y_all.append(data['y_data'])  \n",
    "\n",
    "x_all = np.vstack(x_all)  \n",
    "y_all = np.vstack(y_all)\n",
    "\n",
    "print(f\"Original dataset size: {x_all.shape}, Labels distribution: {np.bincount(y_all.flatten())}\")\n",
    "\n",
    "x_all_over, y_all_over = oversampling(x_all, y_all)\n",
    "\n",
    "print(f\"Oversampled dataset size: {x_all_over.shape}, Labels distribution: {np.bincount(y_all_over.flatten())}\")\n",
    "\n",
    "x_all_over = torch.tensor(x_all_over).float()\n",
    "y_all_over = torch.tensor(y_all_over).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data_train, x_data_val, y_data_train, y_data_val = train_test_split(x_all_over, y_all_over, train_size = 0.8, random_state=42, shuffle=True)\n",
    "\n",
    "x_data_train, Vpca = reduce_matrix(x_data_train, None)\n",
    "y_data_train = adjust_size(x_data_train, y_data_train)\n",
    "\n",
    "x_data_val, _ = reduce_matrix(x_data_val, Vpca)\n",
    "y_data_val = adjust_size(x_data_val, y_data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlessandriniEegDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_data[idx], self.y_data[idx]\n",
    "\n",
    "train_dataset = AlessandriniEegDataset(x_data_train, y_data_train)\n",
    "val_dataset = AlessandriniEegDataset(x_data_val, y_data_val)\n",
    "\n",
    "#train_indices, val_indices = split_train_val(dataset_over, test_size=0.2)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 16])\n",
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(train_loader):\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forse serve per miltiadous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EegDataset(Dataset):\n",
    "    def __init__(self, file_paths):\n",
    "        super().__init__()\n",
    "        self.file_paths = file_paths\n",
    "        self.crops_index = self._compute_crops_index()\n",
    "    \n",
    "    def _compute_crops_index(self):\n",
    "        crops_index = []\n",
    "        for file_idx, file_path in enumerate(self.file_paths):\n",
    "            data = np.load(file_path)\n",
    "            num_crops = len(data['x_data'])\n",
    "            crops_index.extend([(file_idx, crop_idx) for crop_idx in range(num_crops)])\n",
    "        return crops_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.crops_index)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, crop_idx = self.crops_index[idx]\n",
    "        file_path = self.file_paths[file_idx]\n",
    "\n",
    "        data = np.load(file_path)\n",
    "        crop = data['x_data'][crop_idx]\n",
    "        label = data['y_data'][crop_idx]\n",
    "\n",
    "        crop = torch.tensor(crop).float()\n",
    "        label = torch.tensor(label).float().squeeze().unsqueeze(0)\n",
    "        \n",
    "        return crop, label\n",
    "\n",
    "# Usage Example\n",
    "precomputed_dir = \"/home/marta/Documenti/eeg-ml-thesis/output/\"\n",
    "file_paths = [os.path.join(precomputed_dir, f) for f in os.listdir(precomputed_dir) if f.endswith('.npz')]\n",
    "\n",
    "dataset = EegDataset(file_paths)\n",
    "\n",
    "train_indices, val_indices = split_train_val(dataset, test_size=0.2)\n",
    "train_loader = DataLoader(Subset(dataset, train_indices), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(Subset(dataset, val_indices), batch_size=32, shuffle=True)\n",
    "\n",
    "# test_loader = DataLoader(test_dataset, batch_size = 32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
