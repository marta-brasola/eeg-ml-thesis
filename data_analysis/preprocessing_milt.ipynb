{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"/home/marta/Documenti/eeg-ml-thesis/\"\n",
    "os.chdir(path)\n",
    "\n",
    "import torch \n",
    "torch.set_num_threads(4) \n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import r_pca \n",
    "import scipy.io\n",
    "from tqdm import tqdm\n",
    "import datetime \n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_reduction(A, tol, comp = 0):\n",
    "  rpca = False\n",
    "  rpca_mu = 0\n",
    "  multiscale_pca = False\n",
    "\n",
    "  assert(len(A.shape) == 2)\n",
    "  dmin = min(A.shape)\n",
    "  if rpca:\n",
    "    r = r_pca.R_pca(A, mu = rpca_mu)\n",
    "    print('Auto tol:', 1e-7 * r.frobenius_norm(r.D), 'used tol:', tol)\n",
    "    print('mu', r.mu, 'lambda', r.lmbda)\n",
    "    L, S = r.fit(tol = tol, max_iter = 10, iter_print = 1)\n",
    "    global norm_s\n",
    "    norm_s = np.linalg.norm(S, ord='fro')  # for debug\n",
    "    print('||A,L,S||:', np.linalg.norm(A, ord='fro'), np.linalg.norm(L, ord='fro'), np.linalg.norm(S, ord='fro'))\n",
    "    #np.savez_compressed('rpca.npz', pre = A, post = L)\n",
    "  elif multiscale_pca:\n",
    "    print('MSPCA...')\n",
    "    #ms = mspca.MultiscalePCA()\n",
    "    #L = ms.fit_transform(A, wavelet_func='sym4', threshold=0.1, scale = True )\n",
    "    print('saving MAT file and calling Matlab...')\n",
    "    scipy.io.savemat('mspca.mat', {'A': A}, do_compression = True)\n",
    "    os.system('matlab -batch \"mspca(\\'mspca.mat\\')\"')\n",
    "    L = scipy.io.loadmat('mspca.mat')['L'] \n",
    "  else:\n",
    "    \n",
    "    L = A\n",
    "  U, lam, V = np.linalg.svd(L, full_matrices = False)  # V is transposed\n",
    "  assert(U.shape == (A.shape[0], dmin) and lam.shape == (dmin,) and V.shape == (dmin, A.shape[1]))\n",
    "  #np.savetxt('singular_values.csv', lam)\n",
    "  lam_trunc = lam[lam > 0.015 * lam[0]]  # magic number\n",
    "  p = comp if comp else len(lam_trunc)\n",
    "  assert(p <= dmin)\n",
    "  print('PCA truncation', dmin, '->', p)\n",
    "  return L, V.T[:,:p]\n",
    "\n",
    "\n",
    "def precompute_crops(subject_list, \n",
    "                     window, \n",
    "                     overlap, \n",
    "                     DATASET_DIR, \n",
    "                     save_dir, \n",
    "                     csv_file_name,\n",
    "                     task):\n",
    "\n",
    "    base_dir = \"/home/marta/Documenti/eeg-ml-thesis/\"\n",
    "\n",
    "\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    task_class = class_groups[task]\n",
    "    label_mapping = {cls: i for i, cls in enumerate(task_class)}\n",
    "\n",
    "    print(f\"Class Mapping: {label_mapping}\")\n",
    "    mapping_file = os.path.join(base_dir, \"output-milt\", f\"class_mapping_{task}.txt\")\n",
    "\n",
    "    with open(mapping_file, \"w\") as f:\n",
    "        f.write(f\"Task: {task}\\n\")\n",
    "        f.write(f\"Class mapping: {label_mapping}\\n\")\n",
    "\n",
    "    \n",
    "    all_crops = []\n",
    "\n",
    "    for subject_id, category_label in subject_list:\n",
    "        file_path = f\"{DATASET_DIR}/{category_label}/{subject_id}.npy\"\n",
    "\n",
    "        eeg = np.load(file_path).T\n",
    "        scaler = StandardScaler()\n",
    "        eeg = scaler.fit_transform(eeg)\n",
    "\n",
    "        num_columns = eeg.shape[1]\n",
    "        num_windows = (len(eeg) - window) // (window - overlap) + 1\n",
    "\n",
    "        i = 0\n",
    "        for w in range(num_windows):\n",
    "            x_data = eeg[i:i + window]\n",
    "            i += (window - overlap)\n",
    "\n",
    "            crop_filename = f\"{subject_id}_{category_label}_crop{w}.npz\"\n",
    "            crop_save_path = os.path.join(save_dir, crop_filename)\n",
    "\n",
    "            y_label = label_mapping[category_label]\n",
    "            y_data = np.array([[y_label]])\n",
    "\n",
    "            np.savez(crop_save_path, x_data=x_data, y_data=y_data)\n",
    "\n",
    "            all_crops.append((subject_id, w, category_label,y_label, crop_save_path))\n",
    "\n",
    "    train_ind, val_ind = train_test_split(all_crops, train_size=0.75, random_state=42, shuffle=True)\n",
    "\n",
    "    with open(csv_file_name, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"subject\", \"crops_number\", \"category_label\", \"label\", \"split\", \"file_path\"])\n",
    "\n",
    "        for crop in train_ind:\n",
    "            writer.writerow([crop[0], crop[1], crop[2], crop[3], \"train\", crop[4]])\n",
    "        for crop in val_ind:\n",
    "            writer.writerow([crop[0], crop[1], crop[2], crop[3], \"val\", crop[4]])\n",
    "\n",
    "def reduce_matrix(A, V, PCA_COMPONENTS):\n",
    "    # Check the shape of A\n",
    "    if len(A.shape) == 2:\n",
    "        # If A is 2D (w, 16), expand it to (1, w, 16)\n",
    "        A = np.expand_dims(A, axis=0)\n",
    "\n",
    "    # Now A should be 3D: (N, w, 16)\n",
    "    B = np.swapaxes(A, 1, 2)  # Swap axes: (N, 16, w)\n",
    "    C = B.reshape((-1, B.shape[2]))  # Flatten: ((N*16), w)\n",
    "\n",
    "    if V is None:\n",
    "        L, V = pca_reduction(C, 5e-6, comp=PCA_COMPONENTS)\n",
    "\n",
    "    B = C @ V  # Apply PCA: ((N*16), p)\n",
    "    B = B.reshape((A.shape[0], A.shape[2], B.shape[1]))  # Reshape: (N, 16, p)\n",
    "\n",
    "    return np.swapaxes(B, 1, 2), V  # Return: (N, p, 16)\n",
    "\n",
    "\n",
    "def compute_and_save_pca_dataset(csv_file, output_dir, PCA_COMPONENTS):\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure output directory exists\n",
    "\n",
    "    data_info = pd.read_csv(csv_file)\n",
    "\n",
    "    # Select a random subset of the training data\n",
    "    train_rows = data_info[data_info[\"split\"] == \"train\"]\n",
    "    sampled_train_rows = train_rows.sample(frac=0.2, random_state=42)  # Sample randomly\n",
    "\n",
    "    train_data = []\n",
    "    \n",
    "    print(f\"Processing training data for PCA computation on {len(sampled_train_rows)} samples...\")\n",
    "    for _, row in tqdm(sampled_train_rows.iterrows(), \n",
    "                       total=len(sampled_train_rows), \n",
    "                       desc=\"Sampling Training Data\"):\n",
    "        npz_data = np.load(row[\"file_path\"])\n",
    "        x_data = npz_data[\"x_data\"]\n",
    "\n",
    "        if len(x_data.shape) == 2:  \n",
    "            x_data = np.expand_dims(x_data, axis=0)  # Convert (w, 16) → (1, w, 16)\n",
    "\n",
    "        x_data = np.swapaxes(x_data, 1, 2)  # (N, 16, w) → (N, w, 16)\n",
    "        x_data = x_data.reshape((-1, x_data.shape[2]))  # ((N*16), w)\n",
    "\n",
    "        train_data.append(x_data)\n",
    "\n",
    "    train_data = np.vstack(train_data)  # Stack sampled data\n",
    "    _, Vpca = pca_reduction(train_data, tol=5e-6, comp=PCA_COMPONENTS)\n",
    "\n",
    "    print(f\"Computed PCA matrix (Vpca) with shape: {Vpca.shape}\")\n",
    "\n",
    "    return Vpca\n",
    "\n",
    "def apply_pca_to_dataset(csv_file, output_dir, Vpca, PCA_COMPONENTS):\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)  \n",
    "\n",
    "    data_info = pd.read_csv(csv_file)\n",
    "\n",
    "    for _, row in tqdm(data_info.iterrows(), total=len(data_info), desc=\"Transforming Data\"):\n",
    "        npz_data = np.load(row[\"file_path\"])\n",
    "        x_data = npz_data[\"x_data\"]\n",
    "        y_data = npz_data[\"y_data\"]\n",
    "\n",
    "        x_data, _ = reduce_matrix(x_data, Vpca, PCA_COMPONENTS)  \n",
    "\n",
    "        save_path = os.path.join(output_dir, os.path.basename(row[\"file_path\"]))\n",
    "        np.savez(save_path, x_data=x_data, y_data=y_data)\n",
    "        # print(f\"Saved PCA-transformed test file: {save_path}\")\n",
    "\n",
    "def subj_list_task(task, df):\n",
    "    \"\"\"Creates list of subjects to load based on the task\"\"\"\n",
    "\n",
    "    subset = df[df[\"Group\"].isin(class_groups[task])]\n",
    "\n",
    "    subject_list = tuple(zip(subset['participant_id'], subset['Group']))\n",
    "\n",
    "    train, test = train_test_split(subject_list, test_size = 0.1, random_state=42, stratify=subset[\"Group\"])\n",
    "\n",
    "    print(f\"Task: {task}\")\n",
    "    print(f\"Number of Subjects in Train set {len(train)}\")\n",
    "    print(f\"Number of Subjects in Train set {len(test)}\")\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 1000\n",
      "overlap: 250\n",
      "number pca components: 50\n",
      "Task: A_vs_C\n",
      "Number of Subjects in Train set 58\n",
      "Number of Subjects in Train set 7\n",
      "Class Mapping: {'A': 0, 'C': 1}\n",
      "Class Mapping: {'A': 0, 'C': 1}\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = \"/home/marta/Documenti/milt_np_dataset\"\n",
    "OUTPUT_DATA_DIR = \"/home/marta/Documenti/data-milt-preprocessed/\"\n",
    "os.makedirs(OUTPUT_DATA_DIR, exist_ok=True)\n",
    "WINDOW = 1000\n",
    "OVERLAP = WINDOW // 4\n",
    "TASK = \"A_vs_C\"\n",
    "# OVERLAP = 0\n",
    "PCA_COMPONENTS = 50\n",
    "num_epochs = 20\n",
    "print(f\"window: {WINDOW}\")\n",
    "print(f\"overlap: {OVERLAP}\")\n",
    "print(f\"number pca components: {PCA_COMPONENTS}\")\n",
    "\n",
    "## CLASSES\n",
    "# A\t\"Alzheimer Disease Group\"\n",
    "# F\t\"Frontotemporal Dementia Group\"\n",
    "# C\t\"Healthy Group\"\n",
    "\n",
    "# Loading data and computing crops\n",
    "df = pd.read_csv(\"/home/marta/Documenti/milt_dataset/datatset/participants.tsv\",sep=\"\\t\")\n",
    "\n",
    "class_groups = {\n",
    "    \"A_vs_C\": [\"A\", \"C\"],\n",
    "    \"A_vs_F\": [\"A\", \"F\"],\n",
    "    \"F_vs_C\": [\"F\", \"C\"],\n",
    "    \"A_vs_F_vs_C\": [\"A\", \"F\", \"C\"]\n",
    "}\n",
    "# function that creates the list of subjects to load based on the task\n",
    "train_subj_list, test_subj_list = subj_list_task(TASK, df)\n",
    "\n",
    "test_path = os.path.join(OUTPUT_DATA_DIR, f\"test_w{WINDOW}_ovr{OVERLAP}_{TASK}\")\n",
    "train_path = os.path.join(OUTPUT_DATA_DIR, f\"train_w{WINDOW}_ovr{OVERLAP}_{TASK}\")\n",
    "os.makedirs(test_path, exist_ok=True)\n",
    "os.makedirs(train_path, exist_ok=True)\n",
    "\n",
    "\n",
    "train_config = f\"config/train_w{WINDOW}_ovr{OVERLAP}_{TASK}.csv\"\n",
    "test_config = f\"config/test_w{WINDOW}_ovr{OVERLAP}_{TASK}.csv\"\n",
    "\n",
    "precompute_crops(train_subj_list, \n",
    "                 window=WINDOW, \n",
    "                 DATASET_DIR=DATASET_DIR, \n",
    "                 ave_dir=train_path,\n",
    "                 csv_file_name= train_config, \n",
    "                 task=TASK, overlap=OVERLAP)\n",
    "\n",
    "precompute_crops(test_subj_list, \n",
    "                 window=WINDOW, \n",
    "                 DATASET_DIR=DATASET_DIR, \n",
    "                 save_dir=test_path, \n",
    "                 csv_file_name= test_config,\n",
    "                 task=TASK, overlap=OVERLAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data for PCA computation on 4726 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling Training Data: 100%|██████████| 4726/4726 [01:02<00:00, 75.74it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA truncation 1000 -> 50\n",
      "Computed PCA matrix (Vpca) with shape: (1000, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming Data: 100%|██████████| 3933/3933 [04:47<00:00, 13.69it/s]\n",
      "Transforming Data: 100%|██████████| 31508/31508 [38:48<00:00, 13.53it/s]  \n"
     ]
    }
   ],
   "source": [
    " \n",
    "test_pca_path = os.path.join(OUTPUT_DATA_DIR, f\"test_w{WINDOW}_ovr{OVERLAP}_pca{PCA_COMPONENTS}_{TASK}\")\n",
    "train_pca_path = os.path.join(OUTPUT_DATA_DIR, f\"train_w{WINDOW}_ovr{OVERLAP}_pca{PCA_COMPONENTS}_{TASK}\")\n",
    "os.makedirs(test_pca_path, exist_ok=True)\n",
    "os.makedirs(train_pca_path, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "Vpca = compute_and_save_pca_dataset(train_config, train_pca_path, PCA_COMPONENTS=50)\n",
    "\n",
    "\n",
    "apply_pca_to_dataset(test_config, test_pca_path, Vpca, PCA_COMPONENTS=50)\n",
    "apply_pca_to_dataset(train_config, train_pca_path, Vpca, PCA_COMPONENTS=50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
